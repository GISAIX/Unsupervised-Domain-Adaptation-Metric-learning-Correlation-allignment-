{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function, absolute_import\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import numpy\n",
    "import PIL\n",
    "from PIL import Image\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "from math import sqrt\n",
    "\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 715, 102)\n",
      "(1096, 715)\n",
      "(72933, 102)\n",
      "(72933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint16 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaCentre.mat')\n",
    "arr = mat['pavia']\n",
    "arr = np.array(arr)\n",
    "print(arr.shape)\n",
    "\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaCentre_gt.mat')\n",
    "arr1 = mat['pavia_gt']\n",
    "arr1 = np.array(arr1)\n",
    "print(arr1.shape)\n",
    "\n",
    "a=[]\n",
    "label=[]\n",
    "k=0\n",
    "for i in range(0,arr1.shape[0]):\n",
    "    for j in range(0,arr1[i].shape[0]):\n",
    "        a.append(arr[i][j])\n",
    "        label.append(arr1[i][j])\n",
    "        \n",
    "a=np.array(a)\n",
    "label=np.array(label)\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "for i in range (0,a.shape[0]):\n",
    "    if(label[i]==2):\n",
    "        y_train.append(0)\n",
    "    if(label[i]==3):\n",
    "        y_train.append(1)\n",
    "    if(label[i]==4):\n",
    "        y_train.append(2)\n",
    "    if(label[i]==5):\n",
    "        y_train.append(3)\n",
    "    if(label[i]==7):\n",
    "        y_train.append(4)\n",
    "    if(label[i]==8):\n",
    "        y_train.append(5)\n",
    "    if(label[i]==9):\n",
    "        y_train.append(6)\n",
    "    if (label[i]==2 or label[i]==3 or label[i]==4 or label[i]==5 or label[i]==7 or label[i]==8 or label[i]==9):\n",
    "        X_train.append(a[i])\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state = 0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaU.mat')\n",
    "arr = mat['paviaU']\n",
    "arr = np.array(arr)\n",
    "\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaU_gt.mat')\n",
    "arr1 = mat['paviaU_gt']\n",
    "arr1 = np.array(arr1)\n",
    "print(arr1.shape)\n",
    "\n",
    "a=[]\n",
    "label=[]\n",
    "k=0\n",
    "for i in range(0,arr1.shape[0]):\n",
    "    for j in range(0,arr1[i].shape[0]):\n",
    "        a.append(arr[i][j])\n",
    "        label.append(arr1[i][j])\n",
    "        \n",
    "a=np.array(a)\n",
    "label=np.array(label)\n",
    "print(a.shape)\n",
    "print(label.shape)\n",
    "\n",
    "X_train1=[]\n",
    "y_train1=[]\n",
    "for i in range (0,a.shape[0]):\n",
    "    if(label[i]==4):\n",
    "        y_train1.append(0)\n",
    "    if(label[i]==1):\n",
    "        y_train1.append(1)\n",
    "    if(label[i]==8):\n",
    "        y_train1.append(2)\n",
    "    if(label[i]==7):\n",
    "        y_train1.append(3)\n",
    "    if(label[i]==9):\n",
    "        y_train1.append(4)\n",
    "    if(label[i]==2):\n",
    "        y_train1.append(5)\n",
    "    if(label[i]==6):\n",
    "        y_train1.append(6)\n",
    "    if (label[i]==4 or label[i]==1 or label[i]==8 or label[i]==7 or label[i]==9 or label[i]==2 or label[i]==6):\n",
    "        X_train1.append(a[i])\n",
    "X_train1=np.array(X_train1)\n",
    "y_train1=np.array(y_train1)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train1, y_train1 = shuffle(X_train1, y_train1, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train1 = StandardScaler().fit_transform(X_train1)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)\n",
    "X_train1 = pca.fit_transform(X_train1)\n",
    "print(X_train1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.42358570277176\n",
      "19.465860817811212\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train1.max())\n",
    "X_train=X_train.astype('float32')\n",
    "X_train1=X_train1.astype('float32')\n",
    "X_train=X_train/100\n",
    "X_train1=X_train1/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72933, 64)\n",
      "(20000, 64)\n",
      "(19332, 64)\n"
     ]
    }
   ],
   "source": [
    "X_test=X_train1[20000:39332,:]\n",
    "y_test=y_train1[20000:39332]\n",
    "X_train1=X_train1[0:20000,:]\n",
    "y_train1=y_train1[0:20000]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train1.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "num_steps = 20\n",
    "batch_size = 20\n",
    "total_numbers = 291\n",
    "display_step = 1000\n",
    "examples_to_show = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_hidden_1 = 32 # 1st layer num features\n",
    "num_hidden_2 = 16 # 2nd layer num features (the latent dim)\n",
    "num_input = 64 \n",
    "num_classes = 7\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "\n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_uniform([num_input, num_hidden_1], minval=-4*np.sqrt(6.0/(num_input + num_hidden_1)), maxval=4*np.sqrt(6.0/(num_input + num_hidden_1)))),\n",
    "    'encoder_h2': tf.Variable(tf.random_uniform([num_hidden_1, num_hidden_2], minval=-4*np.sqrt(6.0/(num_hidden_1 + num_hidden_2)), maxval=4*np.sqrt(6.0/(num_hidden_1 + num_hidden_2)))),\n",
    "    'decoder_h1': tf.Variable(tf.random_uniform([num_hidden_2, num_hidden_1], minval=-4*np.sqrt(6.0/(num_hidden_1 + num_hidden_2)), maxval=4*np.sqrt(6.0/(num_hidden_1 + num_hidden_2)))),\n",
    "    'decoder_h2': tf.Variable(tf.random_uniform([num_hidden_1, num_input], minval=-4*np.sqrt(6.0/(num_input + num_hidden_1)), maxval=4*np.sqrt(6.0/(num_input + num_hidden_1)))),\n",
    "    'classifier1_h': tf.Variable(tf.random_uniform([num_hidden_2, 10], minval=-4*np.sqrt(6.0/(10 + num_hidden_2)), maxval=4*np.sqrt(6.0/(10 + num_hidden_2)))),\n",
    "    'classifier_h': tf.Variable(tf.random_uniform([10, num_classes], minval=-4*np.sqrt(6.0/(10 + num_classes)), maxval=4*np.sqrt(6.0/(10 + num_classes)))),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.truncated_normal([num_hidden_1])/sqrt(num_hidden_1)),\n",
    "    'encoder_b2': tf.Variable(tf.truncated_normal([num_hidden_2])/sqrt(num_hidden_2)),\n",
    "    'decoder_b1': tf.Variable(tf.truncated_normal([num_hidden_1])/sqrt(num_hidden_1)),\n",
    "    'decoder_b2': tf.Variable(tf.truncated_normal([num_input])/sqrt(num_hidden_2)),\n",
    "    'classifier1_b': tf.Variable(tf.truncated_normal([10])/sqrt(10)),\n",
    "    'classifier_b': tf.Variable(tf.truncated_normal([num_classes])/sqrt(num_classes)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "                                   biases['encoder_b1']))\n",
    "    # Encoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "                                   biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    # Decoder Hidden layer with sigmoid activation #1\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "                                   biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with sigmoid activation #2\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "                                   biases['decoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "classify1 = tf.nn.sigmoid(tf.add(tf.matmul(encoder_op, weights['classifier1_h']), biases['classifier1_b']))\n",
    "label_pred = tf.nn.softmax(tf.add(tf.matmul(classify1, weights['classifier_h']), biases['classifier_b']))\n",
    "y_clipped = tf.clip_by_value(label_pred, 1e-10, 0.9999999)\n",
    "\n",
    "\n",
    "# Targets (Labels) are the input data.\n",
    "y_true = X\n",
    "label_true = Y\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "loss_autoencoder = tf.reduce_mean(tf.pow(y_true - y_pred, 2))\n",
    "cross_entropy_loss = -tf.reduce_mean(tf.reduce_sum(label_true * tf.log(y_clipped)\n",
    "                         + (1 - label_true) * tf.log(1 - y_clipped), axis=1))\n",
    "loss_total = loss_autoencoder+cross_entropy_loss\n",
    "\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss_total)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72933, 7)\n",
      "(19332, 7)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y_test11 = np_utils.to_categorical(y_test)\n",
    "y_train11 = np_utils.to_categorical(y_train)\n",
    "print(y_train11.shape)\n",
    "print(y_test11.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an accuracy assessment operation\n",
    "correct_prediction = tf.equal(tf.argmax(Y, 1), tf.argmax(label_pred, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost = 0.00734950\n",
      "Epoch: 1 accuracy = 0.10417784\n",
      "Epoch: 2 cost = 0.00703827\n",
      "Epoch: 2 accuracy = 0.10417785\n",
      "Epoch: 3 cost = 0.00674415\n",
      "Epoch: 3 accuracy = 0.10417785\n",
      "Epoch: 4 cost = 0.00646713\n",
      "Epoch: 4 accuracy = 0.10417785\n",
      "Epoch: 5 cost = 0.00621176\n",
      "Epoch: 5 accuracy = 0.45842096\n",
      "Epoch: 6 cost = 0.00598614\n",
      "Epoch: 6 accuracy = 0.58719665\n",
      "Epoch: 7 cost = 0.00579824\n",
      "Epoch: 7 accuracy = 0.58719665\n",
      "Epoch: 8 cost = 0.00565139\n",
      "Epoch: 8 accuracy = 0.58719665\n",
      "Epoch: 9 cost = 0.00554237\n",
      "Epoch: 9 accuracy = 0.58719659\n",
      "Epoch: 10 cost = 0.00546329\n",
      "Epoch: 10 accuracy = 0.58719665\n",
      "Epoch: 11 cost = 0.00540497\n",
      "Epoch: 11 accuracy = 0.58719665\n",
      "Epoch: 12 cost = 0.00535950\n",
      "Epoch: 12 accuracy = 0.58719659\n",
      "Epoch: 13 cost = 0.00532120\n",
      "Epoch: 13 accuracy = 0.58719665\n",
      "Epoch: 14 cost = 0.00528649\n",
      "Epoch: 14 accuracy = 0.58719659\n",
      "Epoch: 15 cost = 0.00525334\n",
      "Epoch: 15 accuracy = 0.58719665\n",
      "Epoch: 16 cost = 0.00522071\n",
      "Epoch: 16 accuracy = 0.58719665\n",
      "Epoch: 17 cost = 0.00518810\n",
      "Epoch: 17 accuracy = 0.58719659\n",
      "Epoch: 18 cost = 0.00515525\n",
      "Epoch: 18 accuracy = 0.58719665\n",
      "Epoch: 19 cost = 0.00512206\n",
      "Epoch: 19 accuracy = 0.58719659\n",
      "Epoch: 20 cost = 0.00508851\n",
      "Epoch: 20 accuracy = 0.58719659\n",
      "Epoch: 21 cost = 0.00505460\n",
      "Epoch: 21 accuracy = 0.58719659\n",
      "Epoch: 22 cost = 0.00502041\n",
      "Epoch: 22 accuracy = 0.58719659\n",
      "Epoch: 23 cost = 0.00498602\n",
      "Epoch: 23 accuracy = 0.58719659\n",
      "Epoch: 24 cost = 0.00495158\n",
      "Epoch: 24 accuracy = 0.58719665\n",
      "Epoch: 25 cost = 0.00491725\n",
      "Epoch: 25 accuracy = 0.58719665\n",
      "Epoch: 26 cost = 0.00488322\n",
      "Epoch: 26 accuracy = 0.58719659\n",
      "Epoch: 27 cost = 0.00484970\n",
      "Epoch: 27 accuracy = 0.58719659\n",
      "Epoch: 28 cost = 0.00481693\n",
      "Epoch: 28 accuracy = 0.58719659\n",
      "Epoch: 29 cost = 0.00478511\n",
      "Epoch: 29 accuracy = 0.58719659\n",
      "Epoch: 30 cost = 0.00475443\n",
      "Epoch: 30 accuracy = 0.58719659\n",
      "Epoch: 31 cost = 0.00472501\n",
      "Epoch: 31 accuracy = 0.58719659\n",
      "Epoch: 32 cost = 0.00469691\n",
      "Epoch: 32 accuracy = 0.58719659\n",
      "Epoch: 33 cost = 0.00467014\n",
      "Epoch: 33 accuracy = 0.58719665\n",
      "Epoch: 34 cost = 0.00464463\n",
      "Epoch: 34 accuracy = 0.58719665\n",
      "Epoch: 35 cost = 0.00462028\n",
      "Epoch: 35 accuracy = 0.58719665\n",
      "Epoch: 36 cost = 0.00459699\n",
      "Epoch: 36 accuracy = 0.58719665\n",
      "Epoch: 37 cost = 0.00457464\n",
      "Epoch: 37 accuracy = 0.58719665\n",
      "Epoch: 38 cost = 0.00455313\n",
      "Epoch: 38 accuracy = 0.58719665\n",
      "Epoch: 39 cost = 0.00453237\n",
      "Epoch: 39 accuracy = 0.58719665\n",
      "Epoch: 40 cost = 0.00451226\n",
      "Epoch: 40 accuracy = 0.58719659\n",
      "Epoch: 41 cost = 0.00449269\n",
      "Epoch: 41 accuracy = 0.58719659\n",
      "Epoch: 42 cost = 0.00447352\n",
      "Epoch: 42 accuracy = 0.58719659\n",
      "Epoch: 43 cost = 0.00445459\n",
      "Epoch: 43 accuracy = 0.58719659\n",
      "Epoch: 44 cost = 0.00443575\n",
      "Epoch: 44 accuracy = 0.58719665\n",
      "Epoch: 45 cost = 0.00441684\n",
      "Epoch: 45 accuracy = 0.58719665\n",
      "Epoch: 46 cost = 0.00439775\n",
      "Epoch: 46 accuracy = 0.58719665\n",
      "Epoch: 47 cost = 0.00437838\n",
      "Epoch: 47 accuracy = 0.58719659\n",
      "Epoch: 48 cost = 0.00435867\n",
      "Epoch: 48 accuracy = 0.58719659\n",
      "Epoch: 49 cost = 0.00433860\n",
      "Epoch: 49 accuracy = 0.58719665\n",
      "Epoch: 50 cost = 0.00431817\n",
      "Epoch: 50 accuracy = 0.58719659\n",
      "Epoch: 51 cost = 0.00429740\n",
      "Epoch: 51 accuracy = 0.58719665\n",
      "Epoch: 52 cost = 0.00427642\n",
      "Epoch: 52 accuracy = 0.58719659\n",
      "Epoch: 53 cost = 0.00425546\n",
      "Epoch: 53 accuracy = 0.58719665\n",
      "Epoch: 54 cost = 0.00423482\n",
      "Epoch: 54 accuracy = 0.58719665\n",
      "Epoch: 55 cost = 0.00421483\n",
      "Epoch: 55 accuracy = 0.58719659\n",
      "Epoch: 56 cost = 0.00419565\n",
      "Epoch: 56 accuracy = 0.58719665\n",
      "Epoch: 57 cost = 0.00417708\n",
      "Epoch: 57 accuracy = 0.58719659\n",
      "Epoch: 58 cost = 0.00415861\n",
      "Epoch: 58 accuracy = 0.58719659\n",
      "Epoch: 59 cost = 0.00413994\n",
      "Epoch: 59 accuracy = 0.58719659\n",
      "Epoch: 60 cost = 0.00412137\n",
      "Epoch: 60 accuracy = 0.58719665\n",
      "Epoch: 61 cost = 0.00410364\n",
      "Epoch: 61 accuracy = 0.58719665\n",
      "Epoch: 62 cost = 0.00408702\n",
      "Epoch: 62 accuracy = 0.58719665\n",
      "Epoch: 63 cost = 0.00407089\n",
      "Epoch: 63 accuracy = 0.58719665\n",
      "Epoch: 64 cost = 0.00405456\n",
      "Epoch: 64 accuracy = 0.58719659\n",
      "Epoch: 65 cost = 0.00403835\n",
      "Epoch: 65 accuracy = 0.58719665\n",
      "Epoch: 66 cost = 0.00402330\n",
      "Epoch: 66 accuracy = 0.58719665\n",
      "Epoch: 67 cost = 0.00401002\n",
      "Epoch: 67 accuracy = 0.58719665\n",
      "Epoch: 68 cost = 0.00399812\n",
      "Epoch: 68 accuracy = 0.58719665\n",
      "Epoch: 69 cost = 0.00398677\n",
      "Epoch: 69 accuracy = 0.58719665\n",
      "Epoch: 70 cost = 0.00397511\n",
      "Epoch: 70 accuracy = 0.58719665\n",
      "Epoch: 71 cost = 0.00396224\n",
      "Epoch: 71 accuracy = 0.58719665\n",
      "Epoch: 72 cost = 0.00394739\n",
      "Epoch: 72 accuracy = 0.58719665\n",
      "Epoch: 73 cost = 0.00393204\n",
      "Epoch: 73 accuracy = 0.58719665\n",
      "Epoch: 74 cost = 0.00391756\n",
      "Epoch: 74 accuracy = 0.58719659\n",
      "Epoch: 75 cost = 0.00389540\n",
      "Epoch: 75 accuracy = 0.58719665\n",
      "Epoch: 76 cost = 0.00387800\n",
      "Epoch: 76 accuracy = 0.58719659\n",
      "Epoch: 77 cost = 0.00386795\n",
      "Epoch: 77 accuracy = 0.58719665\n",
      "Epoch: 78 cost = 0.00384533\n",
      "Epoch: 78 accuracy = 0.58719665\n",
      "Epoch: 79 cost = 0.00380471\n",
      "Epoch: 79 accuracy = 0.58719665\n",
      "Epoch: 80 cost = 0.00380091\n",
      "Epoch: 80 accuracy = 0.59365463\n",
      "Epoch: 81 cost = 0.00381877\n",
      "Epoch: 81 accuracy = 0.58738858\n",
      "Epoch: 82 cost = 0.00375925\n",
      "Epoch: 82 accuracy = 0.60105860\n",
      "Epoch: 83 cost = 0.00375068\n",
      "Epoch: 83 accuracy = 0.59464180\n",
      "Epoch: 84 cost = 0.00371068\n",
      "Epoch: 84 accuracy = 0.60868192\n",
      "Epoch: 85 cost = 0.00369473\n",
      "Epoch: 85 accuracy = 0.61396074\n",
      "Epoch: 86 cost = 0.00366969\n",
      "Epoch: 86 accuracy = 0.62576610\n",
      "Epoch: 87 cost = 0.00364753\n",
      "Epoch: 87 accuracy = 0.63988858\n",
      "Epoch: 88 cost = 0.00362765\n",
      "Epoch: 88 accuracy = 0.66811997\n",
      "Epoch: 89 cost = 0.00364898\n",
      "Epoch: 89 accuracy = 0.67522240\n",
      "Epoch: 90 cost = 0.00362291\n",
      "Epoch: 90 accuracy = 0.65487486\n",
      "Epoch: 91 cost = 0.00354625\n",
      "Epoch: 91 accuracy = 0.64915735\n",
      "Epoch: 92 cost = 0.00354189\n",
      "Epoch: 92 accuracy = 0.60964185\n",
      "Epoch: 93 cost = 0.00357716\n",
      "Epoch: 93 accuracy = 0.58921218\n",
      "Epoch: 94 cost = 0.00352429\n",
      "Epoch: 94 accuracy = 0.59998912\n",
      "Epoch: 95 cost = 0.00348231\n",
      "Epoch: 95 accuracy = 0.62102211\n",
      "Epoch: 96 cost = 0.00344937\n",
      "Epoch: 96 accuracy = 0.64519489\n",
      "Epoch: 97 cost = 0.00342493\n",
      "Epoch: 97 accuracy = 0.66968304\n",
      "Epoch: 98 cost = 0.00340641\n",
      "Epoch: 98 accuracy = 0.68701398\n",
      "Epoch: 99 cost = 0.00340171\n",
      "Epoch: 99 accuracy = 0.69057882\n",
      "Epoch: 100 cost = 0.00340610\n",
      "Epoch: 100 accuracy = 0.69082558\n",
      "Epoch: 101 cost = 0.00336995\n",
      "Epoch: 101 accuracy = 0.69059253\n",
      "Epoch: 102 cost = 0.00331886\n",
      "Epoch: 102 accuracy = 0.69005781\n",
      "Epoch: 103 cost = 0.00328028\n",
      "Epoch: 103 accuracy = 0.68826163\n",
      "Epoch: 104 cost = 0.00327666\n",
      "Epoch: 104 accuracy = 0.65413451\n",
      "Epoch: 105 cost = 0.00336887\n",
      "Epoch: 105 accuracy = 0.61605859\n",
      "Epoch: 106 cost = 0.00327727\n",
      "Epoch: 106 accuracy = 0.66784567\n",
      "Epoch: 107 cost = 0.00320443\n",
      "Epoch: 107 accuracy = 0.68331194\n",
      "Epoch: 108 cost = 0.00317716\n",
      "Epoch: 108 accuracy = 0.68864560\n",
      "Epoch: 109 cost = 0.00316214\n",
      "Epoch: 109 accuracy = 0.69038683\n",
      "Epoch: 110 cost = 0.00314960\n",
      "Epoch: 110 accuracy = 0.69099009\n",
      "Epoch: 111 cost = 0.00313104\n",
      "Epoch: 111 accuracy = 0.69108605\n",
      "Epoch: 112 cost = 0.00310386\n",
      "Epoch: 112 accuracy = 0.69099015\n",
      "Epoch: 113 cost = 0.00307527\n",
      "Epoch: 113 accuracy = 0.69044173\n",
      "Epoch: 114 cost = 0.00305970\n",
      "Epoch: 114 accuracy = 0.68613636\n",
      "Epoch: 115 cost = 0.00309499\n",
      "Epoch: 115 accuracy = 0.65853584\n",
      "Epoch: 116 cost = 0.00307998\n",
      "Epoch: 116 accuracy = 0.67520869\n",
      "Epoch: 117 cost = 0.00299960\n",
      "Epoch: 117 accuracy = 0.68870044\n",
      "Epoch: 118 cost = 0.00296607\n",
      "Epoch: 118 accuracy = 0.73008066\n",
      "Epoch: 119 cost = 0.00297968\n",
      "Epoch: 119 accuracy = 0.73057431\n",
      "Epoch: 120 cost = 0.00296570\n",
      "Epoch: 120 accuracy = 0.73053318\n",
      "Epoch: 121 cost = 0.00291912\n",
      "Epoch: 121 accuracy = 0.73043716\n",
      "Epoch: 122 cost = 0.00288671\n",
      "Epoch: 122 accuracy = 0.73027259\n",
      "Epoch: 123 cost = 0.00286779\n",
      "Epoch: 123 accuracy = 0.72987503\n",
      "Epoch: 124 cost = 0.00285409\n",
      "Epoch: 124 accuracy = 0.72917581\n",
      "Epoch: 125 cost = 0.00283935\n",
      "Epoch: 125 accuracy = 0.72877818\n",
      "Epoch: 126 cost = 0.00281574\n",
      "Epoch: 126 accuracy = 0.72935402\n",
      "Epoch: 127 cost = 0.00278790\n",
      "Epoch: 127 accuracy = 0.73034120\n",
      "Epoch: 128 cost = 0.00278879\n",
      "Epoch: 128 accuracy = 0.72905242\n",
      "Epoch: 129 cost = 0.00282093\n",
      "Epoch: 129 accuracy = 0.72946370\n",
      "Epoch: 130 cost = 0.00274759\n",
      "Epoch: 130 accuracy = 0.73030013\n",
      "Epoch: 131 cost = 0.00270320\n",
      "Epoch: 131 accuracy = 0.73072517\n",
      "Epoch: 132 cost = 0.00268934\n",
      "Epoch: 132 accuracy = 0.73193169\n",
      "Epoch: 133 cost = 0.00267506\n",
      "Epoch: 133 accuracy = 0.73387873\n",
      "Epoch: 134 cost = 0.00265337\n",
      "Epoch: 134 accuracy = 0.74092627\n",
      "Epoch: 135 cost = 0.00263764\n",
      "Epoch: 135 accuracy = 0.75625539\n",
      "Epoch: 136 cost = 0.00263191\n",
      "Epoch: 136 accuracy = 0.76219225\n",
      "Epoch: 137 cost = 0.00261329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 137 accuracy = 0.76633304\n",
      "Epoch: 138 cost = 0.00258111\n",
      "Epoch: 138 accuracy = 0.78215569\n",
      "Epoch: 139 cost = 0.00255833\n",
      "Epoch: 139 accuracy = 0.79435855\n",
      "Epoch: 140 cost = 0.00254234\n",
      "Epoch: 140 accuracy = 0.79896557\n",
      "Epoch: 141 cost = 0.00252147\n",
      "Epoch: 141 accuracy = 0.80211920\n",
      "Epoch: 142 cost = 0.00250263\n",
      "Epoch: 142 accuracy = 0.80479294\n",
      "Epoch: 143 cost = 0.00249198\n",
      "Epoch: 143 accuracy = 0.80508083\n",
      "Epoch: 144 cost = 0.00247487\n",
      "Epoch: 144 accuracy = 0.80588984\n",
      "Epoch: 145 cost = 0.00244739\n",
      "Epoch: 145 accuracy = 0.80756259\n",
      "Epoch: 146 cost = 0.00242406\n",
      "Epoch: 146 accuracy = 0.80901593\n",
      "Epoch: 147 cost = 0.00240627\n",
      "Epoch: 147 accuracy = 0.80996209\n",
      "Epoch: 148 cost = 0.00238701\n",
      "Epoch: 148 accuracy = 0.81046933\n",
      "Epoch: 149 cost = 0.00236817\n",
      "Epoch: 149 accuracy = 0.81040084\n",
      "Epoch: 150 cost = 0.00235285\n",
      "Epoch: 150 accuracy = 0.81051046\n",
      "Epoch: 151 cost = 0.00233549\n",
      "Epoch: 151 accuracy = 0.81082582\n",
      "Epoch: 152 cost = 0.00231367\n",
      "Epoch: 152 accuracy = 0.81185412\n",
      "Epoch: 153 cost = 0.00229210\n",
      "Epoch: 153 accuracy = 0.81296480\n",
      "Epoch: 154 cost = 0.00227348\n",
      "Epoch: 154 accuracy = 0.81363660\n",
      "Epoch: 155 cost = 0.00225523\n",
      "Epoch: 155 accuracy = 0.81407547\n",
      "Epoch: 156 cost = 0.00223589\n",
      "Epoch: 156 accuracy = 0.81421256\n",
      "Epoch: 157 cost = 0.00221896\n",
      "Epoch: 157 accuracy = 0.81432211\n",
      "Epoch: 158 cost = 0.00220291\n",
      "Epoch: 158 accuracy = 0.81454146\n",
      "Epoch: 159 cost = 0.00218426\n",
      "Epoch: 159 accuracy = 0.81515849\n",
      "Epoch: 160 cost = 0.00216407\n",
      "Epoch: 160 accuracy = 0.81576180\n",
      "Epoch: 161 cost = 0.00214513\n",
      "Epoch: 161 accuracy = 0.81684500\n",
      "Epoch: 162 cost = 0.00212783\n",
      "Epoch: 162 accuracy = 0.81731123\n",
      "Epoch: 163 cost = 0.00210981\n",
      "Epoch: 163 accuracy = 0.81780469\n",
      "Epoch: 164 cost = 0.00209244\n",
      "Epoch: 164 accuracy = 0.81794196\n",
      "Epoch: 165 cost = 0.00207714\n",
      "Epoch: 165 accuracy = 0.81821615\n",
      "Epoch: 166 cost = 0.00206042\n",
      "Epoch: 166 accuracy = 0.81849039\n",
      "Epoch: 167 cost = 0.00204137\n",
      "Epoch: 167 accuracy = 0.81884682\n",
      "Epoch: 168 cost = 0.00202244\n",
      "Epoch: 168 accuracy = 0.81951869\n",
      "Epoch: 169 cost = 0.00200537\n",
      "Epoch: 169 accuracy = 0.81988883\n",
      "Epoch: 170 cost = 0.00198981\n",
      "Epoch: 170 accuracy = 0.82014942\n",
      "Epoch: 171 cost = 0.00197264\n",
      "Epoch: 171 accuracy = 0.82038254\n",
      "Epoch: 172 cost = 0.00195615\n",
      "Epoch: 172 accuracy = 0.82032758\n",
      "Epoch: 173 cost = 0.00194296\n",
      "Epoch: 173 accuracy = 0.82014942\n",
      "Epoch: 174 cost = 0.00192898\n",
      "Epoch: 174 accuracy = 0.81986141\n",
      "Epoch: 175 cost = 0.00191130\n",
      "Epoch: 175 accuracy = 0.82006705\n",
      "Epoch: 176 cost = 0.00189005\n",
      "Epoch: 176 accuracy = 0.82072526\n",
      "Epoch: 177 cost = 0.00187149\n",
      "Epoch: 177 accuracy = 0.82090342\n",
      "Epoch: 178 cost = 0.00185740\n",
      "Epoch: 178 accuracy = 0.82071161\n",
      "Epoch: 179 cost = 0.00184382\n",
      "Epoch: 179 accuracy = 0.82058823\n",
      "Epoch: 180 cost = 0.00182720\n",
      "Epoch: 180 accuracy = 0.82086241\n",
      "Epoch: 181 cost = 0.00181068\n",
      "Epoch: 181 accuracy = 0.82135606\n",
      "Epoch: 182 cost = 0.00180028\n",
      "Epoch: 182 accuracy = 0.82073897\n",
      "Epoch: 183 cost = 0.00179305\n",
      "Epoch: 183 accuracy = 0.82069772\n",
      "Epoch: 184 cost = 0.00176815\n",
      "Epoch: 184 accuracy = 0.82126004\n",
      "Epoch: 185 cost = 0.00174620\n",
      "Epoch: 185 accuracy = 0.82119149\n",
      "Epoch: 186 cost = 0.00173356\n",
      "Epoch: 186 accuracy = 0.82093096\n",
      "Epoch: 187 cost = 0.00172097\n",
      "Epoch: 187 accuracy = 0.82101327\n",
      "Epoch: 188 cost = 0.00170703\n",
      "Epoch: 188 accuracy = 0.82117766\n",
      "Epoch: 189 cost = 0.00169414\n",
      "Epoch: 189 accuracy = 0.82138342\n",
      "Epoch: 190 cost = 0.00168224\n",
      "Epoch: 190 accuracy = 0.82138330\n",
      "Epoch: 191 cost = 0.00166396\n",
      "Epoch: 191 accuracy = 0.82149309\n",
      "Epoch: 192 cost = 0.00164579\n",
      "Epoch: 192 accuracy = 0.82113653\n",
      "Epoch: 193 cost = 0.00163312\n",
      "Epoch: 193 accuracy = 0.82079387\n",
      "Epoch: 194 cost = 0.00162092\n",
      "Epoch: 194 accuracy = 0.82075262\n",
      "Epoch: 195 cost = 0.00160760\n",
      "Epoch: 195 accuracy = 0.82098579\n",
      "Epoch: 196 cost = 0.00159536\n",
      "Epoch: 196 accuracy = 0.82149303\n",
      "Epoch: 197 cost = 0.00158237\n",
      "Epoch: 197 accuracy = 0.82152045\n",
      "Epoch: 198 cost = 0.00156301\n",
      "Epoch: 198 accuracy = 0.82120520\n",
      "Epoch: 199 cost = 0.00154796\n",
      "Epoch: 199 accuracy = 0.82080758\n",
      "Epoch: 200 cost = 0.00153642\n",
      "Epoch: 200 accuracy = 0.82058817\n"
     ]
    }
   ],
   "source": [
    "# Start Training\n",
    "# Start a new TF session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Run the initializer\n",
    "sess.run(init)\n",
    "batch_size = 64\n",
    "num_batch = 1139\n",
    "\n",
    "# Training\n",
    "for i in range(0,200):\n",
    "    k = 0 \n",
    "    # Prepare Data\n",
    "    # Get the next batch of MNIST data (only images are needed, not labels)\n",
    "    avg_cost = 0\n",
    "    for j in (0,num_batch):\n",
    "        batch_x = X_train[k:k+batch_size,:]\n",
    "        batch_y = y_train11[k:k+batch_size,:]\n",
    "        k += 64\n",
    "        #print(j)\n",
    "\n",
    "    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        _, l = sess.run([optimizer, loss_total], feed_dict={X: batch_x, Y: batch_y})\n",
    "        avg_cost += l / num_batch\n",
    "    \n",
    "    print(\"Epoch:\", (i + 1), \"cost =\", \"{:.8f}\".format(avg_cost))\n",
    "    print(\"Epoch:\", (i + 1), \"accuracy =\", \"{:.8f}\".format(sess.run(accuracy, feed_dict={X: X_train, Y: y_train11})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46141127]\n"
     ]
    }
   ],
   "source": [
    "# on 200 epoch\n",
    "\n",
    "print(sess.run([accuracy], feed_dict={X: X_test, Y: y_test11}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
