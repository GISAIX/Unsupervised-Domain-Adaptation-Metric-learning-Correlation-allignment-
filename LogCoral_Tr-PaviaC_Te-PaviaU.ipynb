{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import numpy\n",
    "import PIL\n",
    "from PIL import Image\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def conc(vects):\n",
    "    x, y = vects\n",
    "    conc1 = concatenate([x,y])\n",
    "    return conc1\n",
    "\n",
    "def conc_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],32)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    x = y_pred[:,0:128]\n",
    "    y = y_pred[:,128:268]\n",
    "    y_pred1 = euclidean_distance(x,y)\n",
    "    p = x\n",
    "    q = y\n",
    "    p = K.clip(p, K.epsilon(), 1)\n",
    "    q = K.clip(q, K.epsilon(), 1)\n",
    "    #y_true1 = y_true[:,0]\n",
    "    #y_true1 = K.reshape(y_true1,(-1,))\n",
    "    #print(y_true1)\n",
    "    #tr_same = y_true[:,1]\n",
    "    #tr_same = K.reshape(tr_same, (-1,))\n",
    "    y_true1 = y_true\n",
    "    tr_same = K.round(y_true/3)\n",
    "    margin = 1\n",
    "    test = 0.001*K.sum(p*K.abs(K.log(p)-K.log(q)), axis=1)\n",
    "\n",
    "    return K.mean((1-tr_same)*(y_true1 * K.square(y_pred1) + (1 - y_true1) * K.square(K.maximum(margin - y_pred1, 0)))\n",
    "                 + (tr_same)*test)\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    x = y_pred[:,0:32]\n",
    "    y = y_pred[:,32:64]\n",
    "    y_pred1 = euclidean_distance(x,y)\n",
    "    y_true1 = y_true\n",
    "    margin = 1\n",
    "    return K.mean(y_true1 * K.square(y_pred1) + (1 - y_true1) * K.square(K.maximum(margin - y_pred1, 0)))\n",
    "\n",
    "def coral_loss(y_true, y_pred):\n",
    "    x = y_pred[:,0:32]\n",
    "    y = y_pred[:,32:64]\n",
    "    n = 32.0\n",
    "    mul1 = K.dot(K.transpose(x),x)\n",
    "    one = x*0+1\n",
    "    mul2 = K.dot(K.transpose(one), x)\n",
    "    sub = K.dot(K.transpose(mul2), mul2)\n",
    "    source = (mul1 - (sub)/n)/(n-1)\n",
    "    source = K.abs(source)\n",
    "    source = K.clip(source, K.epsilon(),10000)\n",
    "    source1 = K.log(source)\n",
    "    \n",
    "    mul11 = K.dot(K.transpose(y),y)\n",
    "    mul21 = K.dot(K.transpose(one), y)\n",
    "    sub1 = K.dot(K.transpose(mul2), mul2)\n",
    "    n = float(n)\n",
    "    target = (mul11 - (sub1)/n)/(n-1)\n",
    "    target = K.abs(target)\n",
    "    target = K.clip(target, K.epsilon(),10000)\n",
    "    target1 = K.log(target)\n",
    "    \n",
    "    return (K.sum(K.dot((source1-target1),(source1-target1)))/(4*32*32.0))\n",
    "       \n",
    "    \n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(7)]) - 1\n",
    "    for d in range(7):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1,7)\n",
    "            dn = (d + inc) % 7\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "def create_addi_pairs(x, y):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for i in range(0,1000):\n",
    "        k1 = random.randrange(0,x.shape[0])\n",
    "        for j in range(0,10):\n",
    "            k2 = random.randrange(0, y.shape[0])\n",
    "            pairs+= [[x[k1],y[k2]]]\n",
    "            labels += [3]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "             \n",
    "            \n",
    "def create_base_network():\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    seq = Sequential()\n",
    "    seq.add(Dense(64, input_shape=(64,), activation='relu'))\n",
    "    #seq.add(Dense(64, activation='relu'))\n",
    "    seq.add(Dense(32, activation='relu'))\n",
    "    seq.add(Dense(32, activation='relu'))\n",
    "    return seq\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return labels[predictions.ravel() < 0.5].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 715, 102)\n",
      "(1096, 715)\n",
      "(72933, 102)\n",
      "(72933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint16 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72933, 64)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaCentre.mat')\n",
    "arr = mat['pavia']\n",
    "arr = np.array(arr)\n",
    "print(arr.shape)\n",
    "\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaCentre_gt.mat')\n",
    "arr1 = mat['pavia_gt']\n",
    "arr1 = np.array(arr1)\n",
    "print(arr1.shape)\n",
    "\n",
    "a=[]\n",
    "label=[]\n",
    "k=0\n",
    "for i in range(0,arr1.shape[0]):\n",
    "    for j in range(0,arr1[i].shape[0]):\n",
    "        a.append(arr[i][j])\n",
    "        label.append(arr1[i][j])\n",
    "        \n",
    "a=np.array(a)\n",
    "label=np.array(label)\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "for i in range (0,a.shape[0]):\n",
    "    if(label[i]==2):\n",
    "        y_train.append(0)\n",
    "    if(label[i]==3):\n",
    "        y_train.append(1)\n",
    "    if(label[i]==4):\n",
    "        y_train.append(2)\n",
    "    if(label[i]==5):\n",
    "        y_train.append(3)\n",
    "    if(label[i]==7):\n",
    "        y_train.append(4)\n",
    "    if(label[i]==8):\n",
    "        y_train.append(5)\n",
    "    if(label[i]==9):\n",
    "        y_train.append(6)\n",
    "    if (label[i]==2 or label[i]==3 or label[i]==4 or label[i]==5 or label[i]==7 or label[i]==8 or label[i]==9):\n",
    "        X_train.append(a[i])\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state = 0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(label.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(610, 340)\n",
      "(207400, 103)\n",
      "(207400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint16 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39332, 64)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaU.mat')\n",
    "arr = mat['paviaU']\n",
    "arr = np.array(arr)\n",
    "\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaU_gt.mat')\n",
    "arr1 = mat['paviaU_gt']\n",
    "arr1 = np.array(arr1)\n",
    "print(arr1.shape)\n",
    "\n",
    "a=[]\n",
    "label=[]\n",
    "k=0\n",
    "for i in range(0,arr1.shape[0]):\n",
    "    for j in range(0,arr1[i].shape[0]):\n",
    "        a.append(arr[i][j])\n",
    "        label.append(arr1[i][j])\n",
    "        \n",
    "a=np.array(a)\n",
    "label=np.array(label)\n",
    "print(a.shape)\n",
    "print(label.shape)\n",
    "\n",
    "X_train1=[]\n",
    "y_train1=[]\n",
    "for i in range (0,a.shape[0]):\n",
    "    if(label[i]==4):\n",
    "        y_train1.append(0)\n",
    "    if(label[i]==1):\n",
    "        y_train1.append(1)\n",
    "    if(label[i]==8):\n",
    "        y_train1.append(2)\n",
    "    if(label[i]==7):\n",
    "        y_train1.append(3)\n",
    "    if(label[i]==9):\n",
    "        y_train1.append(4)\n",
    "    if(label[i]==2):\n",
    "        y_train1.append(5)\n",
    "    if(label[i]==6):\n",
    "        y_train1.append(6)\n",
    "    if (label[i]==4 or label[i]==1 or label[i]==8 or label[i]==7 or label[i]==9 or label[i]==2 or label[i]==6):\n",
    "        X_train1.append(a[i])\n",
    "X_train1=np.array(X_train1)\n",
    "y_train1=np.array(y_train1)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train1, y_train1 = shuffle(X_train1, y_train1, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train1 = StandardScaler().fit_transform(X_train1)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)\n",
    "X_train1 = pca.fit_transform(X_train1)\n",
    "print(X_train1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.423585702771796\n",
      "104.56701566525479\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train1.max())\n",
    "X_train=X_train.astype('float32')\n",
    "X_train1=X_train1.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.61208\n",
      "-19.46586\n"
     ]
    }
   ],
   "source": [
    "print(X_train.min())\n",
    "print(X_train1.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train/100\n",
    "X_train1=X_train1/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 64)\n",
      "(19332, 64)\n"
     ]
    }
   ],
   "source": [
    "X_test=X_train1[20000:39332,:]\n",
    "y_test=y_train1[20000:39332]\n",
    "X_train1=X_train1[0:20000,:]\n",
    "y_train1=y_train1[0:20000]\n",
    "\n",
    "print(X_train1.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47423583\n",
      "0.4411732\n",
      "1.0456702\n",
      "-0.1961208\n",
      "-0.1944266\n"
     ]
    }
   ],
   "source": [
    "print((X_train).max())\n",
    "print(abs(X_train1).max())\n",
    "print(abs(X_test).max())\n",
    "print(X_train.min())\n",
    "print(X_train1.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_indices = [np.where(y_train == i)[0] for i in range(7)]\n",
    "tr_pairs, tr_y = create_pairs(X_train, digit_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1_pairs, tr1_y = create_addi_pairs(X_train, X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37576, 2, 64)\n",
      "(10000, 2, 64)\n"
     ]
    }
   ],
   "source": [
    "print(tr_pairs.shape)\n",
    "print(tr1_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 64)\n",
      "(?, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"la..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# network definition\n",
    "input_dim=X_train.shape[1:]\n",
    "base_network = create_base_network()\n",
    "\n",
    "input_a = Input(shape=input_dim)\n",
    "input_b = Input(shape=input_dim)\n",
    "\n",
    "#input_a=K.reshape(input_a,(28,28,1))\n",
    "#input_b=K.reshape(input_b,(28,28,1))\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "print(input_b.shape)\n",
    "\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(conc, output_shape=conc_shape)([processed_a, processed_b])\n",
    "print(distance.shape)\n",
    "\n",
    "model = Model(input=[input_a, input_b], output=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"se..., inputs=Tensor(\"in...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_model = Model(input = input_a, output = processed_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 276.00 191.00\" width=\"276pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 272,-187 272,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139653283552336 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139653283552336</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 125,-182.5 125,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-160.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139653283552392 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139653283552392</title>\n",
       "<polygon fill=\"none\" points=\"59,-73.5 59,-109.5 208,-109.5 208,-73.5 59,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-87.8\">sequential_1: Sequential</text>\n",
       "</g>\n",
       "<!-- 139653283552336&#45;&gt;139653283552392 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139653283552336-&gt;139653283552392</title>\n",
       "<path d=\"M79.6871,-146.313C88.5826,-137.417 99.5906,-126.409 109.328,-116.672\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111.875,-119.075 116.471,-109.529 106.925,-114.125 111.875,-119.075\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139653283213944 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139653283213944</title>\n",
       "<polygon fill=\"none\" points=\"143,-146.5 143,-182.5 268,-182.5 268,-146.5 143,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205.5\" y=\"-160.8\">input_2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139653283213944&#45;&gt;139653283552392 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139653283213944-&gt;139653283552392</title>\n",
       "<path d=\"M188.071,-146.313C179.05,-137.417 167.887,-126.409 158.012,-116.672\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"160.347,-114.058 150.769,-109.529 155.432,-119.042 160.347,-114.058\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139653274918864 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139653274918864</title>\n",
       "<polygon fill=\"none\" points=\"72,-0.5 72,-36.5 195,-36.5 195,-0.5 72,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-14.8\">lambda_1: Lambda</text>\n",
       "</g>\n",
       "<!-- 139653283552392&#45;&gt;139653274918864 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139653283552392-&gt;139653274918864</title>\n",
       "<path d=\"M133.5,-73.3129C133.5,-65.2895 133.5,-55.5475 133.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137,-46.5288 133.5,-36.5288 130,-46.5289 137,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37576, 64)\n",
      "(37576, 64)\n",
      "(37576,)\n"
     ]
    }
   ],
   "source": [
    "print(tr_pairs[:,0].shape)\n",
    "print(tr_pairs[:,1].shape)\n",
    "print(tr_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 84us/step - loss: 0.0927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 124us/step - loss: 0.0230\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 71us/step - loss: 0.0654\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 66us/step - loss: 0.0272\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 58us/step - loss: 0.0526\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 73us/step - loss: 0.0487\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 60us/step - loss: 0.0476\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 82us/step - loss: 0.0073\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 67us/step - loss: 0.0445\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 89us/step - loss: 0.0296\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 75us/step - loss: 0.0455\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 96us/step - loss: 1.8968e-04\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 79us/step - loss: 0.0419\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 111us/step - loss: 2.2747e-06\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 95us/step - loss: 0.0543\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 179us/step - loss: 4.1480e-06\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 63us/step - loss: 0.0454\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 103us/step - loss: 3.0842e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 67us/step - loss: 0.0508\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 109us/step - loss: 1.8075e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 84us/step - loss: 0.0482\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 139us/step - loss: 2.6069e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 97us/step - loss: 0.0423\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 110us/step - loss: 4.1021e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 76us/step - loss: 0.0473\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 113us/step - loss: 6.7597e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 76us/step - loss: 0.0447\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 141us/step - loss: 7.3296e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 65us/step - loss: 0.0467\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 120us/step - loss: 4.4950e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 67us/step - loss: 0.0516\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 130us/step - loss: 6.6665e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 69us/step - loss: 0.0468\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 131us/step - loss: 3.5354e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 70us/step - loss: 0.0421\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 135us/step - loss: 2.7837e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 71us/step - loss: 0.0398\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 138us/step - loss: 2.4007e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 73us/step - loss: 0.0394\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 142us/step - loss: 2.5899e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 75us/step - loss: 0.0367\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 150us/step - loss: 2.4475e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 76us/step - loss: 0.0353\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 155us/step - loss: 1.6855e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 78us/step - loss: 0.0338\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 162us/step - loss: 1.3380e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 77us/step - loss: 0.0369\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 160us/step - loss: 1.4695e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 77us/step - loss: 0.0381\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 167us/step - loss: 8.9370e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 84us/step - loss: 0.0410\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 175us/step - loss: 1.4264e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 87us/step - loss: 0.0409\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 173us/step - loss: 2.4404e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 81us/step - loss: 0.0386\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 1.0871e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 84us/step - loss: 0.0390\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 182us/step - loss: 1.2674e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 85us/step - loss: 0.0355\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 188us/step - loss: 1.3729e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 86us/step - loss: 0.0357\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 209us/step - loss: 1.0922e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 100us/step - loss: 0.0340\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 198us/step - loss: 1.0468e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 89us/step - loss: 0.0343\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 196us/step - loss: 1.3186e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 88us/step - loss: 0.0400\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 206us/step - loss: 1.1256e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 89us/step - loss: 0.0381\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 207us/step - loss: 1.2417e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 94us/step - loss: 0.0325\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 213us/step - loss: 1.0641e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 93us/step - loss: 0.0350\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 228us/step - loss: 1.0034e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 93us/step - loss: 0.0347\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 223us/step - loss: 1.1542e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 97us/step - loss: 0.0393\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 229us/step - loss: 8.0056e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 97us/step - loss: 0.0348\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 231us/step - loss: 8.8341e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 99us/step - loss: 0.0379\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 233us/step - loss: 1.3589e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 100us/step - loss: 0.0383\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 1.9730e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 102us/step - loss: 0.0346\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 1.0644e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 101us/step - loss: 0.0349\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 248us/step - loss: 1.2966e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 103us/step - loss: 0.0322\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s 255us/step - loss: 6.7925e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 104us/step - loss: 0.0336\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s 253us/step - loss: 1.3547e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 105us/step - loss: 0.0320\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s 259us/step - loss: 1.1291e-08\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37576/37576 [==============================] - 4s 106us/step - loss: 0.0363\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s 266us/step - loss: 5.5694e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 107us/step - loss: 0.0335\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s 268us/step - loss: 7.0085e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 108us/step - loss: 0.0354\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 3s 274us/step - loss: 6.0953e-09\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from keras import optimizers\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rms = RMSprop()\n",
    "for i in range(0,50):\n",
    "     model.compile(loss=triplet_loss, optimizer=rms)\n",
    "     model.fit([tr_pairs[:,0], tr_pairs[:, 1]], tr_y, batch_size=64, nb_epoch=1)\n",
    "     model.compile(loss=coral_loss, optimizer=rms)\n",
    "     model.fit([tr1_pairs[:,0], tr1_pairs[:, 1]], tr1_y, batch_size=64, nb_epoch=1)\n",
    "     \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.0914621 5.780207  1.7777681 4.3746214 3.460174  7.756788  3.12103\n",
      " 6.798503  4.151381  5.274859  5.5441113 2.8024142 6.373274  7.6195\n",
      " 3.2368684 1.723218  0.        4.0794296 0.        6.580445  1.3640492\n",
      " 6.0394287 3.0713005 6.871936  6.340644  7.290547  5.9615498 0.\n",
      " 6.3364334 3.4673524 6.7090907 3.5367029]\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "processed=test_model.predict(X_train1)\n",
    "print(processed[200])\n",
    "print(processed[100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72933, 7)\n",
      "(20000, 7)\n",
      "(19332, 7)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y_tr = np_utils.to_categorical(y_train)\n",
    "y_tr1 = np_utils.to_categorical(y_train1)\n",
    "y_te = np_utils.to_categorical(y_test)\n",
    "num_classes = 7\n",
    "print(y_tr.shape)\n",
    "print(y_tr1.shape)\n",
    "print(y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = 32\n",
    "# define baseline model\n",
    "def baseline_model1():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(32, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72933, 32)\n",
      "(20000, 32)\n",
      "(19332, 32)\n"
     ]
    }
   ],
   "source": [
    "processed_train = test_model.predict(X_train)\n",
    "processed_train1 = test_model.predict(X_train1)\n",
    "processed_test = test_model.predict(X_test)\n",
    "print(processed_train.shape)\n",
    "print(processed_train1.shape)\n",
    "print(processed_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72933 samples, validate on 20000 samples\n",
      "Epoch 1/100\n",
      "72933/72933 [==============================] - 4s 60us/step - loss: 1.3959 - acc: 0.5851 - val_loss: 1.7908 - val_acc: 0.4754\n",
      "Epoch 2/100\n",
      "72933/72933 [==============================] - 3s 38us/step - loss: 1.3764 - acc: 0.5872 - val_loss: 1.8145 - val_acc: 0.4754\n",
      "Epoch 3/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 1.3753 - acc: 0.5872 - val_loss: 1.8002 - val_acc: 0.4754\n",
      "Epoch 4/100\n",
      "72933/72933 [==============================] - 2s 32us/step - loss: 1.3742 - acc: 0.5872 - val_loss: 1.8174 - val_acc: 0.4754\n",
      "Epoch 5/100\n",
      "72933/72933 [==============================] - 2s 34us/step - loss: 1.3713 - acc: 0.5872 - val_loss: 1.8675 - val_acc: 0.4754\n",
      "Epoch 6/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 1.3653 - acc: 0.5872 - val_loss: 1.7428 - val_acc: 0.4754\n",
      "Epoch 7/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 1.3334 - acc: 0.5872 - val_loss: 1.7012 - val_acc: 0.4754\n",
      "Epoch 8/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 1.2378 - acc: 0.5872 - val_loss: 1.6671 - val_acc: 0.4754\n",
      "Epoch 9/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 1.1298 - acc: 0.6034 - val_loss: 1.6930 - val_acc: 0.4734\n",
      "Epoch 10/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 1.0579 - acc: 0.6264 - val_loss: 1.9598 - val_acc: 0.4742\n",
      "Epoch 11/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 1.0226 - acc: 0.6389 - val_loss: 1.8374 - val_acc: 0.4680\n",
      "Epoch 12/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.9873 - acc: 0.6461 - val_loss: 2.0201 - val_acc: 0.4698\n",
      "Epoch 13/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.9582 - acc: 0.6486 - val_loss: 2.0128 - val_acc: 0.4658\n",
      "Epoch 14/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.9327 - acc: 0.6554 - val_loss: 2.1557 - val_acc: 0.4672\n",
      "Epoch 15/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.9161 - acc: 0.6682 - val_loss: 2.3113 - val_acc: 0.4740\n",
      "Epoch 16/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.9118 - acc: 0.6700 - val_loss: 2.3941 - val_acc: 0.4670\n",
      "Epoch 17/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.8980 - acc: 0.6729 - val_loss: 2.4594 - val_acc: 0.4688\n",
      "Epoch 18/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.8804 - acc: 0.6773 - val_loss: 2.3822 - val_acc: 0.4794\n",
      "Epoch 19/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.8817 - acc: 0.6751 - val_loss: 2.6802 - val_acc: 0.4587\n",
      "Epoch 20/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.8818 - acc: 0.6738 - val_loss: 2.5537 - val_acc: 0.4658\n",
      "Epoch 21/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.8612 - acc: 0.6782 - val_loss: 2.2563 - val_acc: 0.5113\n",
      "Epoch 22/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.8576 - acc: 0.6783 - val_loss: 2.2212 - val_acc: 0.5222\n",
      "Epoch 23/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.8482 - acc: 0.6796 - val_loss: 2.4435 - val_acc: 0.4874\n",
      "Epoch 24/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.8524 - acc: 0.6763 - val_loss: 2.4669 - val_acc: 0.4787\n",
      "Epoch 25/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.8310 - acc: 0.6820 - val_loss: 2.4000 - val_acc: 0.4979\n",
      "Epoch 26/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.8414 - acc: 0.6774 - val_loss: 2.3464 - val_acc: 0.5040\n",
      "Epoch 27/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.8387 - acc: 0.6789 - val_loss: 2.3352 - val_acc: 0.5118\n",
      "Epoch 28/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.8233 - acc: 0.6827 - val_loss: 2.2891 - val_acc: 0.5284\n",
      "Epoch 29/100\n",
      "72933/72933 [==============================] - 2s 32us/step - loss: 0.8213 - acc: 0.6819 - val_loss: 2.3101 - val_acc: 0.5161\n",
      "Epoch 30/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.8213 - acc: 0.6803 - val_loss: 2.3207 - val_acc: 0.5073\n",
      "Epoch 31/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.8187 - acc: 0.6813 - val_loss: 2.4169 - val_acc: 0.4888\n",
      "Epoch 32/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.8038 - acc: 0.6828 - val_loss: 2.4216 - val_acc: 0.4829\n",
      "Epoch 33/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.8088 - acc: 0.6828 - val_loss: 2.5230 - val_acc: 0.4791\n",
      "Epoch 34/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.8072 - acc: 0.6825 - val_loss: 2.4695 - val_acc: 0.4829\n",
      "Epoch 35/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.8051 - acc: 0.6826 - val_loss: 2.3765 - val_acc: 0.4829\n",
      "Epoch 36/100\n",
      "72933/72933 [==============================] - 2s 32us/step - loss: 0.8034 - acc: 0.6818 - val_loss: 2.4632 - val_acc: 0.4795\n",
      "Epoch 37/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7896 - acc: 0.6862 - val_loss: 2.6174 - val_acc: 0.4810\n",
      "Epoch 38/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7957 - acc: 0.6828 - val_loss: 2.4417 - val_acc: 0.4810\n",
      "Epoch 39/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7884 - acc: 0.6866 - val_loss: 2.5337 - val_acc: 0.4839\n",
      "Epoch 40/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7884 - acc: 0.6874 - val_loss: 2.4968 - val_acc: 0.4822\n",
      "Epoch 41/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7979 - acc: 0.6855 - val_loss: 2.3866 - val_acc: 0.4763\n",
      "Epoch 42/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7804 - acc: 0.6883 - val_loss: 2.6913 - val_acc: 0.4794\n",
      "Epoch 43/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7795 - acc: 0.6904 - val_loss: 2.4817 - val_acc: 0.4787\n",
      "Epoch 44/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7787 - acc: 0.6938 - val_loss: 2.5459 - val_acc: 0.4780\n",
      "Epoch 45/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7886 - acc: 0.6909 - val_loss: 2.4813 - val_acc: 0.4809\n",
      "Epoch 46/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7724 - acc: 0.6972 - val_loss: 2.6163 - val_acc: 0.4798\n",
      "Epoch 47/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7707 - acc: 0.6973 - val_loss: 2.6521 - val_acc: 0.4736\n",
      "Epoch 48/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7568 - acc: 0.7046 - val_loss: 2.7738 - val_acc: 0.4713\n",
      "Epoch 49/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7698 - acc: 0.7010 - val_loss: 2.5702 - val_acc: 0.4792\n",
      "Epoch 50/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7629 - acc: 0.7029 - val_loss: 2.5896 - val_acc: 0.4769\n",
      "Epoch 51/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7628 - acc: 0.7046 - val_loss: 2.4987 - val_acc: 0.4743\n",
      "Epoch 52/100\n",
      "72933/72933 [==============================] - 2s 32us/step - loss: 0.7543 - acc: 0.7080 - val_loss: 2.4901 - val_acc: 0.4743\n",
      "Epoch 53/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7409 - acc: 0.7127 - val_loss: 2.4421 - val_acc: 0.4897\n",
      "Epoch 54/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7495 - acc: 0.7088 - val_loss: 2.5894 - val_acc: 0.4750\n",
      "Epoch 55/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7658 - acc: 0.7004 - val_loss: 2.4031 - val_acc: 0.4768\n",
      "Epoch 56/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7503 - acc: 0.7034 - val_loss: 2.3752 - val_acc: 0.4746\n",
      "Epoch 57/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7576 - acc: 0.7040 - val_loss: 2.6790 - val_acc: 0.4749\n",
      "Epoch 58/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7585 - acc: 0.7049 - val_loss: 2.4196 - val_acc: 0.4757\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7654 - acc: 0.7030 - val_loss: 2.8338 - val_acc: 0.4682\n",
      "Epoch 60/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7520 - acc: 0.7072 - val_loss: 2.6267 - val_acc: 0.4769\n",
      "Epoch 61/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7450 - acc: 0.7097 - val_loss: 2.5882 - val_acc: 0.4775\n",
      "Epoch 62/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7405 - acc: 0.7139 - val_loss: 2.5363 - val_acc: 0.4752\n",
      "Epoch 63/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7411 - acc: 0.7136 - val_loss: 2.5621 - val_acc: 0.4763\n",
      "Epoch 64/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7441 - acc: 0.7132 - val_loss: 2.5628 - val_acc: 0.4779\n",
      "Epoch 65/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7407 - acc: 0.7148 - val_loss: 2.8172 - val_acc: 0.4714\n",
      "Epoch 66/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7526 - acc: 0.7125 - val_loss: 2.5137 - val_acc: 0.4763\n",
      "Epoch 67/100\n",
      "72933/72933 [==============================] - 2s 32us/step - loss: 0.7399 - acc: 0.7180 - val_loss: 2.6660 - val_acc: 0.4781\n",
      "Epoch 68/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7295 - acc: 0.7205 - val_loss: 2.5185 - val_acc: 0.4750\n",
      "Epoch 69/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7424 - acc: 0.7172 - val_loss: 2.5824 - val_acc: 0.4804\n",
      "Epoch 70/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7226 - acc: 0.7225 - val_loss: 3.0948 - val_acc: 0.4484\n",
      "Epoch 71/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7328 - acc: 0.7183 - val_loss: 2.5321 - val_acc: 0.4757\n",
      "Epoch 72/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7177 - acc: 0.7250 - val_loss: 2.6658 - val_acc: 0.4782\n",
      "Epoch 73/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7390 - acc: 0.7147 - val_loss: 2.3916 - val_acc: 0.4772\n",
      "Epoch 74/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7302 - acc: 0.7180 - val_loss: 2.3832 - val_acc: 0.4715\n",
      "Epoch 75/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7450 - acc: 0.7127 - val_loss: 2.3778 - val_acc: 0.4763\n",
      "Epoch 76/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7485 - acc: 0.7121 - val_loss: 2.6056 - val_acc: 0.4693\n",
      "Epoch 77/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7382 - acc: 0.7159 - val_loss: 2.4138 - val_acc: 0.4748\n",
      "Epoch 78/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7362 - acc: 0.7160 - val_loss: 2.4552 - val_acc: 0.4750\n",
      "Epoch 79/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7284 - acc: 0.7195 - val_loss: 2.5560 - val_acc: 0.4773\n",
      "Epoch 80/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7310 - acc: 0.7183 - val_loss: 2.5582 - val_acc: 0.4698\n",
      "Epoch 81/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7352 - acc: 0.7164 - val_loss: 2.4691 - val_acc: 0.4400\n",
      "Epoch 82/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7308 - acc: 0.7196 - val_loss: 2.5182 - val_acc: 0.4637\n",
      "Epoch 83/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7207 - acc: 0.7232 - val_loss: 2.6405 - val_acc: 0.4696\n",
      "Epoch 84/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7426 - acc: 0.7154 - val_loss: 2.6209 - val_acc: 0.4693\n",
      "Epoch 85/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7292 - acc: 0.7195 - val_loss: 2.6169 - val_acc: 0.4781\n",
      "Epoch 86/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7217 - acc: 0.7223 - val_loss: 2.6160 - val_acc: 0.4691\n",
      "Epoch 87/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7175 - acc: 0.7229 - val_loss: 2.5546 - val_acc: 0.4759\n",
      "Epoch 88/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7202 - acc: 0.7244 - val_loss: 2.5875 - val_acc: 0.4673\n",
      "Epoch 89/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7229 - acc: 0.7230 - val_loss: 2.5796 - val_acc: 0.4753\n",
      "Epoch 90/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7224 - acc: 0.7226 - val_loss: 2.5684 - val_acc: 0.4601\n",
      "Epoch 91/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7142 - acc: 0.7256 - val_loss: 2.5645 - val_acc: 0.4626\n",
      "Epoch 92/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7175 - acc: 0.7250 - val_loss: 2.6163 - val_acc: 0.4644\n",
      "Epoch 93/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7227 - acc: 0.7229 - val_loss: 2.7423 - val_acc: 0.4607\n",
      "Epoch 94/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7095 - acc: 0.7278 - val_loss: 2.9361 - val_acc: 0.4501\n",
      "Epoch 95/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.7129 - acc: 0.7273 - val_loss: 2.6034 - val_acc: 0.4698\n",
      "Epoch 96/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7060 - acc: 0.7279 - val_loss: 2.7069 - val_acc: 0.4648\n",
      "Epoch 97/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7224 - acc: 0.7228 - val_loss: 2.6052 - val_acc: 0.4717\n",
      "Epoch 98/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7139 - acc: 0.7261 - val_loss: 2.6805 - val_acc: 0.4669\n",
      "Epoch 99/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.6947 - acc: 0.7326 - val_loss: 2.5782 - val_acc: 0.4735\n",
      "Epoch 100/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.7027 - acc: 0.7303 - val_loss: 2.6172 - val_acc: 0.4665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f02da68e5c0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# build the model\n",
    "model1 = baseline_model1()\n",
    "# Fit the model\n",
    "model1.fit(processed_train, y_tr, validation_data=(processed_train1, y_tr1), epochs=100, batch_size=128, verbose=1)\n",
    "# Final evaluation of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 4s 56us/step\n",
      "19332/19332 [==============================] - 1s 58us/step\n",
      "* Accuracy on training set: 77.18%\n",
      "* Accuracy on test set: 42.16%\n"
     ]
    }
   ],
   "source": [
    "#logCORAL\n",
    "scores_train = model1.evaluate(processed_train, y_tr, verbose=1)\n",
    "scores_test = model1.evaluate(processed_test, y_te, verbose=1)\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * scores_train[1]))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 5s 70us/step\n",
      "19332/19332 [==============================] - 1s 68us/step\n",
      "* Accuracy on training set: 98.71%\n",
      "* Accuracy on test set: 43.58%\n"
     ]
    }
   ],
   "source": [
    "#CORAL\n",
    "scores_train = model1.evaluate(processed_train, y_tr, verbose=1)\n",
    "scores_test = model1.evaluate(processed_test, y_te, verbose=1)\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * scores_train[1]))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 1s 20us/step\n",
      "19332/19332 [==============================] - 0s 25us/step\n",
      "* Accuracy on training set: 58.72%\n",
      "* Accuracy on test set: 47.28%\n"
     ]
    }
   ],
   "source": [
    "#64 output dim\n",
    "#CORAL\n",
    "scores_train = model1.evaluate(processed_train, y_tr, verbose=1)\n",
    "scores_test = model1.evaluate(processed_test, y_te, verbose=1)\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * scores_train[1]))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 3s 44us/step\n",
      "19332/19332 [==============================] - 1s 44us/step\n",
      "* Accuracy on training set: 99.27%\n",
      "* Accuracy on test set: 46.63%\n"
     ]
    }
   ],
   "source": [
    "#32 output dim new\n",
    "#CORAL\n",
    "# acc 47.72\n",
    "scores_train = model1.evaluate(processed_train, y_tr, verbose=1)\n",
    "scores_test = model1.evaluate(processed_test, y_te, verbose=1)\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * scores_train[1]))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 3s 43us/step\n",
      "19332/19332 [==============================] - 1s 41us/step\n",
      "* Accuracy on training set: 71.23%\n",
      "* Accuracy on test set: 46.26%\n"
     ]
    }
   ],
   "source": [
    "#logCORAL new\n",
    "#acc 47.81\n",
    "scores_train = model1.evaluate(processed_train, y_tr, verbose=1)\n",
    "scores_test = model1.evaluate(processed_test, y_te, verbose=1)\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * scores_train[1]))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * scores_test[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
