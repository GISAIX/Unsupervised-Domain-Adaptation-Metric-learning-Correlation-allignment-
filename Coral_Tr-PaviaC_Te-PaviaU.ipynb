{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import numpy\n",
    "import PIL\n",
    "from PIL import Image\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import random\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "from keras.layers import Concatenate, Dense, LSTM, Input, concatenate\n",
    "\n",
    "\n",
    "def euclidean_distance(x, y):\n",
    "    return K.sqrt(K.sum(K.square(x - y), axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def eucl_dist_output_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0], 1)\n",
    "\n",
    "def conc(vects):\n",
    "    x, y = vects\n",
    "    conc1 = concatenate([x,y])\n",
    "    return conc1\n",
    "\n",
    "def conc_shape(shapes):\n",
    "    shape1, shape2 = shapes\n",
    "    return (shape1[0],32)\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    '''Contrastive loss from Hadsell-et-al.'06\n",
    "    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    '''\n",
    "    x = y_pred[:,0:128]\n",
    "    y = y_pred[:,128:268]\n",
    "    y_pred1 = euclidean_distance(x,y)\n",
    "    p = x\n",
    "    q = y\n",
    "    p = K.clip(p, K.epsilon(), 1)\n",
    "    q = K.clip(q, K.epsilon(), 1)\n",
    "    #y_true1 = y_true[:,0]\n",
    "    #y_true1 = K.reshape(y_true1,(-1,))\n",
    "    #print(y_true1)\n",
    "    #tr_same = y_true[:,1]\n",
    "    #tr_same = K.reshape(tr_same, (-1,))\n",
    "    y_true1 = y_true\n",
    "    tr_same = K.round(y_true/3)\n",
    "    margin = 1\n",
    "    test = 0.001*K.sum(p*K.abs(K.log(p)-K.log(q)), axis=1)\n",
    "\n",
    "    return K.mean((1-tr_same)*(y_true1 * K.square(y_pred1) + (1 - y_true1) * K.square(K.maximum(margin - y_pred1, 0)))\n",
    "                 + (tr_same)*test)\n",
    "\n",
    "def triplet_loss(y_true, y_pred):\n",
    "    x = y_pred[:,0:32]\n",
    "    y = y_pred[:,32:64]\n",
    "    y_pred1 = euclidean_distance(x,y)\n",
    "    y_true1 = y_true\n",
    "    margin = 1\n",
    "    return K.mean(y_true1 * K.square(y_pred1) + (1 - y_true1) * K.square(K.maximum(margin - y_pred1, 0)))\n",
    "\n",
    "def coral_loss(y_true, y_pred):\n",
    "    x = y_pred[:,0:32]\n",
    "    y = y_pred[:,32:64]\n",
    "    n = 32.0\n",
    "    mul1 = K.dot(K.transpose(x),x)\n",
    "    one = x*0+1\n",
    "    mul2 = K.dot(K.transpose(one), x)\n",
    "    sub = K.dot(K.transpose(mul2), mul2)\n",
    "    source = (mul1 - (sub)/n)/(n-1)\n",
    "    #source = K.abs(source)\n",
    "    source = K.clip(source, K.epsilon(),10000)\n",
    "    #source1 = K.log(source)\n",
    "    \n",
    "    mul11 = K.dot(K.transpose(y),y)\n",
    "    mul21 = K.dot(K.transpose(one), y)\n",
    "    sub1 = K.dot(K.transpose(mul2), mul2)\n",
    "    n = float(n)\n",
    "    target = (mul11 - (sub1)/n)/(n-1)\n",
    "    #target = K.abs(target)\n",
    "    target = K.clip(target, K.epsilon(),10000)\n",
    "    #target1 = K.log(target)\n",
    "    \n",
    "    return (K.sum(K.dot((source-target),(source-target)))/(4*32.0))\n",
    "       \n",
    "    \n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "    '''Positive and negative pair creation.\n",
    "    Alternates between positive and negative pairs.\n",
    "    '''\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    n = min([len(digit_indices[d]) for d in range(7)]) - 1\n",
    "    for d in range(7):\n",
    "        for i in range(n):\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            inc = random.randrange(1,7)\n",
    "            dn = (d + inc) % 7\n",
    "            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "            pairs += [[x[z1], x[z2]]]\n",
    "            labels += [1, 0]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "\n",
    "def create_addi_pairs(x, y):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for i in range(0,1000):\n",
    "        k1 = random.randrange(0,x.shape[0])\n",
    "        for j in range(0,10):\n",
    "            k2 = random.randrange(0, y.shape[0])\n",
    "            pairs+= [[x[k1],y[k2]]]\n",
    "            labels += [3]\n",
    "    return np.array(pairs), np.array(labels)\n",
    "             \n",
    "            \n",
    "def create_base_network():\n",
    "    '''Base network to be shared (eq. to feature extraction).\n",
    "    '''\n",
    "    seq = Sequential()\n",
    "    seq.add(Dense(64, input_shape=(64,), activation='relu'))\n",
    "    #seq.add(Dense(64, activation='relu'))\n",
    "    seq.add(Dense(32, activation='relu'))\n",
    "    seq.add(Dense(32, activation='relu'))\n",
    "    return seq\n",
    "\n",
    "\n",
    "def compute_accuracy(predictions, labels):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    return labels[predictions.ravel() < 0.5].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1096, 715, 102)\n",
      "(1096, 715)\n",
      "(72933, 102)\n",
      "(72933,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint16 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72933, 64)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaCentre.mat')\n",
    "arr = mat['pavia']\n",
    "arr = np.array(arr)\n",
    "print(arr.shape)\n",
    "\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaCentre_gt.mat')\n",
    "arr1 = mat['pavia_gt']\n",
    "arr1 = np.array(arr1)\n",
    "print(arr1.shape)\n",
    "\n",
    "a=[]\n",
    "label=[]\n",
    "k=0\n",
    "for i in range(0,arr1.shape[0]):\n",
    "    for j in range(0,arr1[i].shape[0]):\n",
    "        a.append(arr[i][j])\n",
    "        label.append(arr1[i][j])\n",
    "        \n",
    "a=np.array(a)\n",
    "label=np.array(label)\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "for i in range (0,a.shape[0]):\n",
    "    if(label[i]==2):\n",
    "        y_train.append(0)\n",
    "    if(label[i]==3):\n",
    "        y_train.append(1)\n",
    "    if(label[i]==4):\n",
    "        y_train.append(2)\n",
    "    if(label[i]==5):\n",
    "        y_train.append(3)\n",
    "    if(label[i]==7):\n",
    "        y_train.append(4)\n",
    "    if(label[i]==8):\n",
    "        y_train.append(5)\n",
    "    if(label[i]==9):\n",
    "        y_train.append(6)\n",
    "    if (label[i]==2 or label[i]==3 or label[i]==4 or label[i]==5 or label[i]==7 or label[i]==8 or label[i]==9):\n",
    "        X_train.append(a[i])\n",
    "X_train=np.array(X_train)\n",
    "y_train=np.array(y_train)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(X_train, y_train, random_state = 0)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaCentre1.csv\", X_train,delimiter=\",\" )\n",
    "np.savetxt(\"/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaCentre_gt1.csv\", y_train,delimiter=\",\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(label.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(610, 340)\n",
      "(207400, 103)\n",
      "(207400,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/sklearn/utils/validation.py:475: DataConversionWarning: Data with input dtype uint16 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39332, 64)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaU.mat')\n",
    "arr = mat['paviaU']\n",
    "arr = np.array(arr)\n",
    "\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaU_gt.mat')\n",
    "arr1 = mat['paviaU_gt']\n",
    "arr1 = np.array(arr1)\n",
    "print(arr1.shape)\n",
    "\n",
    "a=[]\n",
    "label=[]\n",
    "k=0\n",
    "for i in range(0,arr1.shape[0]):\n",
    "    for j in range(0,arr1[i].shape[0]):\n",
    "        a.append(arr[i][j])\n",
    "        label.append(arr1[i][j])\n",
    "        \n",
    "a=np.array(a)\n",
    "label=np.array(label)\n",
    "print(a.shape)\n",
    "print(label.shape)\n",
    "\n",
    "X_train1=[]\n",
    "y_train1=[]\n",
    "for i in range (0,a.shape[0]):\n",
    "    if(label[i]==4):\n",
    "        y_train1.append(0)\n",
    "    if(label[i]==1):\n",
    "        y_train1.append(1)\n",
    "    if(label[i]==8):\n",
    "        y_train1.append(2)\n",
    "    if(label[i]==7):\n",
    "        y_train1.append(3)\n",
    "    if(label[i]==9):\n",
    "        y_train1.append(4)\n",
    "    if(label[i]==2):\n",
    "        y_train1.append(5)\n",
    "    if(label[i]==6):\n",
    "        y_train1.append(6)\n",
    "    if (label[i]==4 or label[i]==1 or label[i]==8 or label[i]==7 or label[i]==9 or label[i]==2 or label[i]==6):\n",
    "        X_train1.append(a[i])\n",
    "X_train1=np.array(X_train1)\n",
    "y_train1=np.array(y_train1)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X_train1, y_train1 = shuffle(X_train1, y_train1, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train1 = StandardScaler().fit_transform(X_train1)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=64)\n",
    "X_train1 = pca.fit_transform(X_train1)\n",
    "print(X_train1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaU1.csv\", X_train1,delimiter=\",\" )\n",
    "np.savetxt(\"/home/aniruddha/deep-learning-projects/Siamese_Networks/Dataset/PaviaU_gt1.csv\", y_train1,delimiter=\",\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.423585702771796\n",
      "104.56701566525479\n"
     ]
    }
   ],
   "source": [
    "print(X_train.max())\n",
    "print(X_train1.max())\n",
    "X_train=X_train.astype('float32')\n",
    "X_train1=X_train1.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-19.61208\n",
      "-19.46586\n"
     ]
    }
   ],
   "source": [
    "print(X_train.min())\n",
    "print(X_train1.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train/100\n",
    "X_train1=X_train1/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 64)\n",
      "(19332, 64)\n"
     ]
    }
   ],
   "source": [
    "X_test=X_train1[20000:39332,:]\n",
    "y_test=y_train1[20000:39332]\n",
    "X_train1=X_train1[0:20000,:]\n",
    "y_train1=y_train1[0:20000]\n",
    "\n",
    "print(X_train1.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47423583\n",
      "0.4411732\n",
      "1.0456702\n",
      "-0.1961208\n",
      "-0.1944266\n"
     ]
    }
   ],
   "source": [
    "print((X_train).max())\n",
    "print(abs(X_train1).max())\n",
    "print(abs(X_test).max())\n",
    "print(X_train.min())\n",
    "print(X_train1.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_indices = [np.where(y_train == i)[0] for i in range(7)]\n",
    "tr_pairs, tr_y = create_pairs(X_train, digit_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr1_pairs, tr1_y = create_addi_pairs(X_train, X_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37576, 2, 64)\n",
      "(10000, 2, 64)\n"
     ]
    }
   ],
   "source": [
    "print(tr_pairs.shape)\n",
    "print(tr1_pairs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 64)\n",
      "(?, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:22: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"la..., inputs=[<tf.Tenso...)`\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# network definition\n",
    "input_dim=X_train.shape[1:]\n",
    "base_network = create_base_network()\n",
    "\n",
    "input_a = Input(shape=input_dim)\n",
    "input_b = Input(shape=input_dim)\n",
    "\n",
    "#input_a=K.reshape(input_a,(28,28,1))\n",
    "#input_b=K.reshape(input_b,(28,28,1))\n",
    "# because we re-use the same instance `base_network`,\n",
    "# the weights of the network\n",
    "# will be shared across the two branches\n",
    "print(input_b.shape)\n",
    "\n",
    "processed_a = base_network(input_a)\n",
    "processed_b = base_network(input_b)\n",
    "\n",
    "distance = Lambda(conc, output_shape=conc_shape)([processed_a, processed_b])\n",
    "print(distance.shape)\n",
    "\n",
    "model = Model(input=[input_a, input_b], output=distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"se..., inputs=Tensor(\"in...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_model = Model(input = input_a, output = processed_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 276.00 191.00\" width=\"276pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 272,-187 272,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139964383373688 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139964383373688</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 125,-182.5 125,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"62.5\" y=\"-160.8\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139964383302600 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139964383302600</title>\n",
       "<polygon fill=\"none\" points=\"59,-73.5 59,-109.5 208,-109.5 208,-73.5 59,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-87.8\">sequential_1: Sequential</text>\n",
       "</g>\n",
       "<!-- 139964383373688&#45;&gt;139964383302600 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139964383373688-&gt;139964383302600</title>\n",
       "<path d=\"M79.6871,-146.313C88.5826,-137.417 99.5906,-126.409 109.328,-116.672\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"111.875,-119.075 116.471,-109.529 106.925,-114.125 111.875,-119.075\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139964367549104 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139964367549104</title>\n",
       "<polygon fill=\"none\" points=\"143,-146.5 143,-182.5 268,-182.5 268,-146.5 143,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"205.5\" y=\"-160.8\">input_2: InputLayer</text>\n",
       "</g>\n",
       "<!-- 139964367549104&#45;&gt;139964383302600 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139964367549104-&gt;139964383302600</title>\n",
       "<path d=\"M188.071,-146.313C179.05,-137.417 167.887,-126.409 158.012,-116.672\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"160.347,-114.058 150.769,-109.529 155.432,-119.042 160.347,-114.058\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139964367580800 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139964367580800</title>\n",
       "<polygon fill=\"none\" points=\"72,-0.5 72,-36.5 195,-36.5 195,-0.5 72,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"133.5\" y=\"-14.8\">lambda_1: Lambda</text>\n",
       "</g>\n",
       "<!-- 139964383302600&#45;&gt;139964367580800 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139964383302600-&gt;139964367580800</title>\n",
       "<path d=\"M133.5,-73.3129C133.5,-65.2895 133.5,-55.5475 133.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"137,-46.5288 133.5,-36.5288 130,-46.5289 137,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37576, 64)\n",
      "(37576, 64)\n",
      "(37576,)\n"
     ]
    }
   ],
   "source": [
    "print(tr_pairs[:,0].shape)\n",
    "print(tr_pairs[:,1].shape)\n",
    "print(tr_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 71us/step - loss: 0.0922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aniruddha/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel_launcher.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 1.5502e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 49us/step - loss: 0.0636\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 60us/step - loss: 1.4326e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 58us/step - loss: 0.0501\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 78us/step - loss: 6.1621e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 53us/step - loss: 0.0438\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 68us/step - loss: 1.2498e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 53us/step - loss: 0.0397\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 73us/step - loss: 2.0334e-13\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 55us/step - loss: 0.0367\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 86us/step - loss: 8.3828e-11\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 56us/step - loss: 0.0344\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 91us/step - loss: 9.4876e-11\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 60us/step - loss: 0.0321\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 85us/step - loss: 7.4079e-06\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 62us/step - loss: 0.0303\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 90us/step - loss: 5.3649e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 62us/step - loss: 0.0289\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 99us/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 65us/step - loss: 0.0275\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 97us/step - loss: 4.8741e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 65us/step - loss: 0.0263\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 100us/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 2s 62us/step - loss: 0.0251\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 107us/step - loss: 2.4937e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 83us/step - loss: 0.0246\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 110us/step - loss: 1.2422e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 73us/step - loss: 0.0235\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 112us/step - loss: 6.4463e-12\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 67us/step - loss: 0.0226\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 117us/step - loss: 1.8256e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 67us/step - loss: 0.0216\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 121us/step - loss: 4.0982e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 68us/step - loss: 0.0212\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 135us/step - loss: 2.2520e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 72us/step - loss: 0.0204\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 128us/step - loss: 3.3809e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 70us/step - loss: 0.0196\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 133us/step - loss: 6.1483e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 71us/step - loss: 0.0192\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 138us/step - loss: 3.6259e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 72us/step - loss: 0.0186\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 141us/step - loss: 4.7361e-11\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 74us/step - loss: 0.0182\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 1s 143us/step - loss: 1.1984e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 74us/step - loss: 0.0175\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 152us/step - loss: 7.6319e-12\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 75us/step - loss: 0.0172\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 151us/step - loss: 2.7887e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 75us/step - loss: 0.0169\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 157us/step - loss: 1.1068e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 75us/step - loss: 0.0166\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 159us/step - loss: 1.2286e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 78us/step - loss: 0.0162\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 161us/step - loss: 2.1847e-06\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 78us/step - loss: 0.0156\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 168us/step - loss: 2.7641e-11\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 80us/step - loss: 0.0154\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 169us/step - loss: 7.9111e-12\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 81us/step - loss: 0.0151\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 177us/step - loss: 8.8297e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 82us/step - loss: 0.0149\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 176us/step - loss: 1.1034e-06\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 84us/step - loss: 0.0144\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 187us/step - loss: 1.8890e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 84us/step - loss: 0.0143\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 183us/step - loss: 1.4400e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 85us/step - loss: 0.0138\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 190us/step - loss: 1.4266e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 88us/step - loss: 0.0138\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 197us/step - loss: 1.9166e-11\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 88us/step - loss: 0.0135\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 199us/step - loss: 3.1208e-09\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 90us/step - loss: 0.0131\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 206us/step - loss: 3.5259e-11\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 92us/step - loss: 0.0131\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 206us/step - loss: 5.4339e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 92us/step - loss: 0.0133\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 211us/step - loss: 5.4986e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 92us/step - loss: 0.0128\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 215us/step - loss: 1.5965e-08\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 3s 92us/step - loss: 0.0125\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 219us/step - loss: 6.8198e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 96us/step - loss: 0.0123\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 220us/step - loss: 2.0457e-07\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 95us/step - loss: 0.0121\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 228us/step - loss: 1.0615e-10\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 98us/step - loss: 0.0118\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 227us/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 98us/step - loss: 0.0116\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 232us/step - loss: 2.0780e-13\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 99us/step - loss: 0.0115\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 239us/step - loss: 1.4685e-13\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 100us/step - loss: 0.0113\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 241us/step - loss: 5.1049e-13\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 101us/step - loss: 0.0112\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 243us/step - loss: 0.0000e+00\n",
      "Epoch 1/1\n",
      "37576/37576 [==============================] - 4s 102us/step - loss: 0.0110\n",
      "Epoch 1/1\n",
      "10000/10000 [==============================] - 2s 249us/step - loss: 5.8940e-13\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "from keras import optimizers\n",
    "#sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "rms = RMSprop()\n",
    "for i in range(0,50):\n",
    "     model.compile(loss=triplet_loss, optimizer=rms)\n",
    "     model.fit([tr_pairs[:,0], tr_pairs[:, 1]], tr_y, batch_size=64, nb_epoch=1)\n",
    "     model.compile(loss=coral_loss, optimizer=rms)\n",
    "     model.fit([tr1_pairs[:,0], tr1_pairs[:, 1]], tr1_y, batch_size=64, nb_epoch=1)\n",
    "     \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2657632  0.38469705 0.45765868 0.         0.40599066 0.38079298\n",
      " 0.31975287 0.         0.32441968 0.         0.30818826 0.46019822\n",
      " 0.01177879 0.2807541  0.06151529 0.         0.         0.15642351\n",
      " 0.         0.         0.3702855  0.4606436  0.43249655 0.29456252\n",
      " 0.2968273  0.36736238 0.03106957 0.         0.         0.34328732\n",
      " 0.3173918  0.41384408]\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "processed=test_model.predict(X_train1)\n",
    "print(processed[100])\n",
    "print(processed[100].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72933, 7)\n",
      "(20000, 7)\n",
      "(19332, 7)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "y_tr = np_utils.to_categorical(y_train)\n",
    "y_tr1 = np_utils.to_categorical(y_train1)\n",
    "y_te = np_utils.to_categorical(y_test)\n",
    "num_classes = 7\n",
    "print(y_tr.shape)\n",
    "print(y_tr1.shape)\n",
    "print(y_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pixels = 32\n",
    "# define baseline model\n",
    "def baseline_model1():\n",
    "\t# create model\n",
    "\tmodel = Sequential()\n",
    "\tmodel.add(Dense(32, input_dim=num_pixels, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(16, kernel_initializer='normal', activation='relu'))\n",
    "\tmodel.add(Dense(num_classes, kernel_initializer='normal', activation='softmax'))\n",
    "\t# Compile model\n",
    "\tmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72933, 32)\n",
      "(20000, 32)\n",
      "(19332, 32)\n"
     ]
    }
   ],
   "source": [
    "processed_train = test_model.predict(X_train)\n",
    "processed_train1 = test_model.predict(X_train1)\n",
    "processed_test = test_model.predict(X_test)\n",
    "print(processed_train.shape)\n",
    "print(processed_train1.shape)\n",
    "print(processed_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 72933 samples, validate on 20000 samples\n",
      "Epoch 1/100\n",
      "72933/72933 [==============================] - 4s 59us/step - loss: 0.5285 - acc: 0.8567 - val_loss: 5.6973 - val_acc: 0.4688\n",
      "Epoch 2/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0378 - acc: 0.9919 - val_loss: 6.3764 - val_acc: 0.4710\n",
      "Epoch 3/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0331 - acc: 0.9920 - val_loss: 6.6271 - val_acc: 0.4703\n",
      "Epoch 4/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0321 - acc: 0.9918 - val_loss: 6.7839 - val_acc: 0.4733\n",
      "Epoch 5/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0317 - acc: 0.9920 - val_loss: 6.8547 - val_acc: 0.4714\n",
      "Epoch 6/100\n",
      "72933/72933 [==============================] - 2s 32us/step - loss: 0.0313 - acc: 0.9922 - val_loss: 6.9403 - val_acc: 0.4701\n",
      "Epoch 7/100\n",
      "72933/72933 [==============================] - 3s 34us/step - loss: 0.0310 - acc: 0.9922 - val_loss: 6.9827 - val_acc: 0.4707\n",
      "Epoch 8/100\n",
      "72933/72933 [==============================] - 3s 37us/step - loss: 0.0309 - acc: 0.9921 - val_loss: 6.9720 - val_acc: 0.4704\n",
      "Epoch 9/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0305 - acc: 0.9921 - val_loss: 6.9818 - val_acc: 0.4698\n",
      "Epoch 10/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0302 - acc: 0.9922 - val_loss: 7.0367 - val_acc: 0.4705\n",
      "Epoch 11/100\n",
      "72933/72933 [==============================] - 2s 34us/step - loss: 0.0301 - acc: 0.9923 - val_loss: 7.0966 - val_acc: 0.4718\n",
      "Epoch 12/100\n",
      "72933/72933 [==============================] - 2s 33us/step - loss: 0.0298 - acc: 0.9923 - val_loss: 7.1193 - val_acc: 0.4699\n",
      "Epoch 13/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0296 - acc: 0.9922 - val_loss: 7.1447 - val_acc: 0.4703\n",
      "Epoch 14/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0294 - acc: 0.9923 - val_loss: 7.1212 - val_acc: 0.4693\n",
      "Epoch 15/100\n",
      "72933/72933 [==============================] - 2s 32us/step - loss: 0.0292 - acc: 0.9923 - val_loss: 7.0740 - val_acc: 0.4729\n",
      "Epoch 16/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0290 - acc: 0.9923 - val_loss: 7.0958 - val_acc: 0.4721\n",
      "Epoch 17/100\n",
      "72933/72933 [==============================] - 3s 34us/step - loss: 0.0289 - acc: 0.9923 - val_loss: 7.0887 - val_acc: 0.4711\n",
      "Epoch 18/100\n",
      "72933/72933 [==============================] - 2s 34us/step - loss: 0.0287 - acc: 0.9923 - val_loss: 7.0147 - val_acc: 0.4691\n",
      "Epoch 19/100\n",
      "72933/72933 [==============================] - 2s 33us/step - loss: 0.0284 - acc: 0.9923 - val_loss: 7.0028 - val_acc: 0.4711\n",
      "Epoch 20/100\n",
      "72933/72933 [==============================] - 2s 32us/step - loss: 0.0283 - acc: 0.9924 - val_loss: 7.0858 - val_acc: 0.4713\n",
      "Epoch 21/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0282 - acc: 0.9924 - val_loss: 7.0533 - val_acc: 0.4744\n",
      "Epoch 22/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0281 - acc: 0.9924 - val_loss: 6.9797 - val_acc: 0.4728\n",
      "Epoch 23/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0280 - acc: 0.9925 - val_loss: 6.9967 - val_acc: 0.4726\n",
      "Epoch 24/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0281 - acc: 0.9923 - val_loss: 7.0086 - val_acc: 0.4730\n",
      "Epoch 25/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0279 - acc: 0.9924 - val_loss: 6.9865 - val_acc: 0.4742\n",
      "Epoch 26/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0280 - acc: 0.9925 - val_loss: 7.0224 - val_acc: 0.4728\n",
      "Epoch 27/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0279 - acc: 0.9923 - val_loss: 6.9486 - val_acc: 0.4711\n",
      "Epoch 28/100\n",
      "72933/72933 [==============================] - 3s 41us/step - loss: 0.0278 - acc: 0.9924 - val_loss: 6.9524 - val_acc: 0.4739\n",
      "Epoch 29/100\n",
      "72933/72933 [==============================] - 3s 40us/step - loss: 0.0279 - acc: 0.9924 - val_loss: 6.9607 - val_acc: 0.4723\n",
      "Epoch 30/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0278 - acc: 0.9924 - val_loss: 6.8912 - val_acc: 0.4724\n",
      "Epoch 31/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0277 - acc: 0.9924 - val_loss: 6.9479 - val_acc: 0.4713\n",
      "Epoch 32/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0277 - acc: 0.9926 - val_loss: 6.9128 - val_acc: 0.4757\n",
      "Epoch 33/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0277 - acc: 0.9923 - val_loss: 6.8855 - val_acc: 0.4735\n",
      "Epoch 34/100\n",
      "72933/72933 [==============================] - 2s 32us/step - loss: 0.0278 - acc: 0.9923 - val_loss: 6.8634 - val_acc: 0.4746\n",
      "Epoch 35/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0274 - acc: 0.9924 - val_loss: 6.9030 - val_acc: 0.4717\n",
      "Epoch 36/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0274 - acc: 0.9924 - val_loss: 6.8958 - val_acc: 0.4748\n",
      "Epoch 37/100\n",
      "72933/72933 [==============================] - 2s 28us/step - loss: 0.0275 - acc: 0.9923 - val_loss: 6.9348 - val_acc: 0.4727\n",
      "Epoch 38/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0275 - acc: 0.9925 - val_loss: 6.9109 - val_acc: 0.4752\n",
      "Epoch 39/100\n",
      "72933/72933 [==============================] - 2s 28us/step - loss: 0.0274 - acc: 0.9923 - val_loss: 6.8893 - val_acc: 0.4746\n",
      "Epoch 40/100\n",
      "72933/72933 [==============================] - 2s 28us/step - loss: 0.0274 - acc: 0.9926 - val_loss: 6.9160 - val_acc: 0.4758\n",
      "Epoch 41/100\n",
      "72933/72933 [==============================] - 2s 28us/step - loss: 0.0273 - acc: 0.9923 - val_loss: 6.8523 - val_acc: 0.4712\n",
      "Epoch 42/100\n",
      "72933/72933 [==============================] - 2s 28us/step - loss: 0.0273 - acc: 0.9925 - val_loss: 6.8936 - val_acc: 0.4748\n",
      "Epoch 43/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0273 - acc: 0.9926 - val_loss: 6.8733 - val_acc: 0.4770\n",
      "Epoch 44/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0273 - acc: 0.9925 - val_loss: 6.8396 - val_acc: 0.4757\n",
      "Epoch 45/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0273 - acc: 0.9924 - val_loss: 6.8942 - val_acc: 0.4730\n",
      "Epoch 46/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0272 - acc: 0.9925 - val_loss: 6.8252 - val_acc: 0.4747\n",
      "Epoch 47/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0273 - acc: 0.9923 - val_loss: 6.8426 - val_acc: 0.4759\n",
      "Epoch 48/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0273 - acc: 0.9924 - val_loss: 6.8202 - val_acc: 0.4735\n",
      "Epoch 49/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0272 - acc: 0.9925 - val_loss: 6.8497 - val_acc: 0.4738\n",
      "Epoch 50/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0271 - acc: 0.9925 - val_loss: 6.9117 - val_acc: 0.4733\n",
      "Epoch 51/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0271 - acc: 0.9925 - val_loss: 6.8523 - val_acc: 0.4752\n",
      "Epoch 52/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0271 - acc: 0.9924 - val_loss: 6.8429 - val_acc: 0.4750\n",
      "Epoch 53/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0270 - acc: 0.9926 - val_loss: 6.8654 - val_acc: 0.4738\n",
      "Epoch 54/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0270 - acc: 0.9925 - val_loss: 6.8586 - val_acc: 0.4749\n",
      "Epoch 55/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0271 - acc: 0.9924 - val_loss: 6.8701 - val_acc: 0.4745\n",
      "Epoch 56/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0270 - acc: 0.9924 - val_loss: 6.8240 - val_acc: 0.4732\n",
      "Epoch 57/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0271 - acc: 0.9924 - val_loss: 6.8710 - val_acc: 0.4733\n",
      "Epoch 58/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0271 - acc: 0.9924 - val_loss: 6.8536 - val_acc: 0.4740\n",
      "Epoch 59/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0269 - acc: 0.9925 - val_loss: 6.8507 - val_acc: 0.4743\n",
      "Epoch 60/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0271 - acc: 0.9924 - val_loss: 6.8297 - val_acc: 0.4760\n",
      "Epoch 61/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0268 - acc: 0.9926 - val_loss: 6.8577 - val_acc: 0.4738\n",
      "Epoch 62/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0268 - acc: 0.9926 - val_loss: 6.8568 - val_acc: 0.4731\n",
      "Epoch 63/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0268 - acc: 0.9925 - val_loss: 6.8129 - val_acc: 0.4738\n",
      "Epoch 64/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0270 - acc: 0.9924 - val_loss: 6.8340 - val_acc: 0.4749\n",
      "Epoch 65/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0268 - acc: 0.9925 - val_loss: 6.8764 - val_acc: 0.4766\n",
      "Epoch 66/100\n",
      "72933/72933 [==============================] - 2s 28us/step - loss: 0.0267 - acc: 0.9925 - val_loss: 6.9001 - val_acc: 0.4736\n",
      "Epoch 67/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0268 - acc: 0.9924 - val_loss: 6.8300 - val_acc: 0.4776\n",
      "Epoch 68/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0267 - acc: 0.9924 - val_loss: 6.8457 - val_acc: 0.4757\n",
      "Epoch 69/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0267 - acc: 0.9925 - val_loss: 6.8478 - val_acc: 0.4742\n",
      "Epoch 70/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0266 - acc: 0.9926 - val_loss: 6.8626 - val_acc: 0.4750\n",
      "Epoch 71/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0268 - acc: 0.9922 - val_loss: 6.8645 - val_acc: 0.4742\n",
      "Epoch 72/100\n",
      "72933/72933 [==============================] - 2s 28us/step - loss: 0.0266 - acc: 0.9924 - val_loss: 6.8288 - val_acc: 0.4733\n",
      "Epoch 73/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0267 - acc: 0.9925 - val_loss: 6.9147 - val_acc: 0.4765\n",
      "Epoch 74/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0266 - acc: 0.9925 - val_loss: 6.8697 - val_acc: 0.4739\n",
      "Epoch 75/100\n",
      "72933/72933 [==============================] - 2s 29us/step - loss: 0.0265 - acc: 0.9925 - val_loss: 6.9205 - val_acc: 0.4723\n",
      "Epoch 76/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0265 - acc: 0.9925 - val_loss: 6.9309 - val_acc: 0.4738\n",
      "Epoch 77/100\n",
      "72933/72933 [==============================] - 3s 36us/step - loss: 0.0265 - acc: 0.9923 - val_loss: 6.9095 - val_acc: 0.4716\n",
      "Epoch 78/100\n",
      "72933/72933 [==============================] - 2s 34us/step - loss: 0.0264 - acc: 0.9926 - val_loss: 6.8582 - val_acc: 0.4772\n",
      "Epoch 79/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0266 - acc: 0.9925 - val_loss: 6.8842 - val_acc: 0.4726\n",
      "Epoch 80/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0265 - acc: 0.9925 - val_loss: 6.9008 - val_acc: 0.4753\n",
      "Epoch 81/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0265 - acc: 0.9924 - val_loss: 6.9007 - val_acc: 0.4726\n",
      "Epoch 82/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0264 - acc: 0.9924 - val_loss: 6.8909 - val_acc: 0.4746\n",
      "Epoch 83/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0263 - acc: 0.9925 - val_loss: 6.9698 - val_acc: 0.4718\n",
      "Epoch 84/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0265 - acc: 0.9925 - val_loss: 6.8784 - val_acc: 0.4721\n",
      "Epoch 85/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0264 - acc: 0.9927 - val_loss: 6.8664 - val_acc: 0.4719\n",
      "Epoch 86/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0262 - acc: 0.9924 - val_loss: 6.9113 - val_acc: 0.4718\n",
      "Epoch 87/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0262 - acc: 0.9924 - val_loss: 6.9000 - val_acc: 0.4741\n",
      "Epoch 88/100\n",
      "72933/72933 [==============================] - 2s 30us/step - loss: 0.0265 - acc: 0.9925 - val_loss: 6.9031 - val_acc: 0.4717\n",
      "Epoch 89/100\n",
      "72933/72933 [==============================] - 2s 33us/step - loss: 0.0263 - acc: 0.9924 - val_loss: 6.9179 - val_acc: 0.4719\n",
      "Epoch 90/100\n",
      "72933/72933 [==============================] - 3s 37us/step - loss: 0.0263 - acc: 0.9926 - val_loss: 6.8678 - val_acc: 0.4749\n",
      "Epoch 91/100\n",
      "72933/72933 [==============================] - 3s 40us/step - loss: 0.0263 - acc: 0.9924 - val_loss: 6.8973 - val_acc: 0.4750\n",
      "Epoch 92/100\n",
      "72933/72933 [==============================] - 3s 41us/step - loss: 0.0262 - acc: 0.9926 - val_loss: 6.9217 - val_acc: 0.4726\n",
      "Epoch 93/100\n",
      "72933/72933 [==============================] - 3s 43us/step - loss: 0.0264 - acc: 0.9926 - val_loss: 6.8260 - val_acc: 0.4713\n",
      "Epoch 94/100\n",
      "72933/72933 [==============================] - 2s 33us/step - loss: 0.0263 - acc: 0.9924 - val_loss: 6.9288 - val_acc: 0.4728\n",
      "Epoch 95/100\n",
      "72933/72933 [==============================] - 2s 31us/step - loss: 0.0262 - acc: 0.9927 - val_loss: 6.8294 - val_acc: 0.4746\n",
      "Epoch 96/100\n",
      "72933/72933 [==============================] - 3s 37us/step - loss: 0.0261 - acc: 0.9924 - val_loss: 6.9662 - val_acc: 0.4711\n",
      "Epoch 97/100\n",
      "72933/72933 [==============================] - 4s 50us/step - loss: 0.0261 - acc: 0.9927 - val_loss: 6.9129 - val_acc: 0.4743\n",
      "Epoch 98/100\n",
      "72933/72933 [==============================] - 4s 50us/step - loss: 0.0261 - acc: 0.9927 - val_loss: 6.9296 - val_acc: 0.4715\n",
      "Epoch 99/100\n",
      "72933/72933 [==============================] - 4s 56us/step - loss: 0.0260 - acc: 0.9927 - val_loss: 6.8563 - val_acc: 0.4720\n",
      "Epoch 100/100\n",
      "72933/72933 [==============================] - 5s 73us/step - loss: 0.0261 - acc: 0.9925 - val_loss: 6.9205 - val_acc: 0.4713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4b47afbf98>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# build the model\n",
    "model1 = baseline_model1()\n",
    "# Fit the model\n",
    "model1.fit(processed_train, y_tr, validation_data=(processed_train1, y_tr1), epochs=100, batch_size=128, verbose=1)\n",
    "# Final evaluation of the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 4s 56us/step\n",
      "19332/19332 [==============================] - 1s 58us/step\n",
      "* Accuracy on training set: 77.18%\n",
      "* Accuracy on test set: 42.16%\n"
     ]
    }
   ],
   "source": [
    "#logCORAL\n",
    "scores_train = model1.evaluate(processed_train, y_tr, verbose=1)\n",
    "scores_test = model1.evaluate(processed_test, y_te, verbose=1)\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * scores_train[1]))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 5s 70us/step\n",
      "19332/19332 [==============================] - 1s 68us/step\n",
      "* Accuracy on training set: 98.71%\n",
      "* Accuracy on test set: 43.58%\n"
     ]
    }
   ],
   "source": [
    "#CORAL\n",
    "scores_train = model1.evaluate(processed_train, y_tr, verbose=1)\n",
    "scores_test = model1.evaluate(processed_test, y_te, verbose=1)\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * scores_train[1]))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 1s 20us/step\n",
      "19332/19332 [==============================] - 0s 25us/step\n",
      "* Accuracy on training set: 58.72%\n",
      "* Accuracy on test set: 47.28%\n"
     ]
    }
   ],
   "source": [
    "#64 output dim\n",
    "#CORAL\n",
    "scores_train = model1.evaluate(processed_train, y_tr, verbose=1)\n",
    "scores_test = model1.evaluate(processed_test, y_te, verbose=1)\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * scores_train[1]))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72933/72933 [==============================] - 3s 44us/step\n",
      "19332/19332 [==============================] - 1s 44us/step\n",
      "* Accuracy on training set: 99.27%\n",
      "* Accuracy on test set: 46.63%\n"
     ]
    }
   ],
   "source": [
    "#32 output dim new\n",
    "#CORAL\n",
    "# acc 47.72\n",
    "scores_train = model1.evaluate(processed_train, y_tr, verbose=1)\n",
    "scores_test = model1.evaluate(processed_test, y_te, verbose=1)\n",
    "print('* Accuracy on training set: %0.2f%%' % (100 * scores_train[1]))\n",
    "print('* Accuracy on test set: %0.2f%%' % (100 * scores_test[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
